% !TEX root = ../main.tex

\begin{abstract}[zh]
随着云计算与人工智能技术的深度融合，现代云存储系统所承载的工作负载正经历深刻变革，呈现出传统数据与 AI 模型数据共存的混合新常态。在此背景下，作为数据缩减核心技术的传统重复数据删除方法，因其粗粒度的冗余识别机制，在应对新兴数据特征时效能急剧下降，面临严峻挑战。

本研究深入剖析了传统技术失效的根源，明确了两种影响显著的新型细粒度冗余模式：其一，AI 模型数据中的“数值相似性冗余”，即浮点数表示中指数位高位的高度重复，传统字节级比对无法感知；其二，大尺寸传统数据块中的“局部连续冗余”，即块内存在的大量重复数据片段，粗粒度的块级去重对此视而不见。更为严峻的是，为单一场景设计的优化方案在混合数据流中会相互干扰，产生“负优化”。因此，开发一套能够智能识别并协同处理多种冗余模式的统一框架，是突破当前技术瓶颈的关键。

为解决上述挑战，本论文设计并实现了一套面向混合负载的细粒度冗余识别重复数据删除系统 FHRD (Fine-grained Hybrid Redundancy Deduplication)。系统的核心创新在于构建了一个统一的、自适应的数据处理框架。其关键是提出了一种基于内容特征的高精度块级数据类型鉴别方法，该方法摒弃了不可靠的文件元数据，通过实时分析数据块字节层面呈现的“循环相似模式”，能够快速、准确地识别出数据块蕴含的冗余类型，并能精准识别出模型文件中的非模型部分，避免误处理。

该框架根据鉴别结果，在统一的流水线中自适应地调用最优的差异化冗余处理模块。对于识别出的模型数据块，系统采用专用的浮点数编码器，通过分离并重组高度相似的指数位部分，将不可压缩的数值冗余转化为高密度可压缩的数据表征。对于识别出的传统数据块，系统则调用增强的去重模块，通过高效的取整子块切分匹配算法，发现并消除其中的长连续重复序列，显著提升单一大块数据的内部去重率。

我们在开源重复数据删除框架 Destor 上实现了完整的 FHRD 系统原型，并进行了全面的实验评估。实验结果表明，在各类混合负载下，FHRD 的去重率相较于传统方法取得了 20\%--30\% 的显著提升，且随着模型数据占比和块尺寸的增大优势愈发明显。同时，系统整体开销与基线相比处于同一量级，体现了其轻量级设计的高效性。

综上所述，本研究不仅精准诊断了传统去重技术在 AI 时代面临的瓶颈，更创新性地提出并验证了一套以细粒度冗余识别和自适应处理为核心的完整解决方案，为应对当前及未来云存储环境下的数据缩减难题提供了有力的技术支撑。
  % 摘要页的下方注明本文的关键词（4 \textasciitilde{} 6 个）。
\end{abstract}

\begin{abstract}[en]
As cloud computing and artificial intelligence continue to deeply integrate, the workloads hosted by modern cloud storage systems are undergoing a profound transformation, leading to a new normal of mixed environments where traditional data and AI model data coexist. In this context, traditional data deduplication methods, as a core technology for data reduction, are facing severe challenges due to their coarse-grained redundancy identification mechanisms, which result in a sharp decline in effectiveness when dealing with emerging data characteristics.

This study delves into the root causes of the failure of traditional techniques, identifying two significant new types of fine-grained redundancy patterns. The first is "numerical similarity redundancy" in AI model data, characterized by the high repetition of the high-order bits in the exponent of floating-point representations, which is imperceptible to traditional byte-level comparisons. The second is "local continuous redundancy" within large-sized traditional data chunks, where a significant amount of repetitive data segments within blocks is overlooked by coarse-grained, block-level deduplication. More critically, optimization solutions designed for single scenarios interfere with each other in mixed data streams, leading to "negative optimization." Therefore, developing a unified framework capable of intelligently identifying and synergistically processing multiple redundancy patterns is key to overcoming the current technological bottleneck.

To address these challenges, this thesis designs and implements a fine-grained redundancy identification deduplication system for mixed workloads, named FHRD (Fine-grained Hybrid Redundancy Deduplication). The core innovation of the system lies in the construction of a unified, adaptive data processing framework. A key aspect of this is the proposal of a high-precision, block-level data type identification method based on content features. This method abandons unreliable file metadata and, by performing real-time analysis of the "cyclic similarity patterns" present at the byte level of data chunks, can quickly and accurately identify the type of redundancy contained within them. It can also precisely identify non-model parts within model files, thus avoiding mis-processing.

Based on the identification results, the framework adaptively invokes the optimal differentiated redundancy processing module within a unified pipeline. For identified model data chunks, the system employs a specialized floating-point encoder that transforms in-compressible numerical redundancy into a highly compressible data representation by separating and reorganizing the highly similar exponent parts. For identified traditional data chunks, the system calls an enhanced deduplication module that discovers and eliminates long continuous repetitive sequences through an efficient rounding sub-block chunking matching algorithm, significantly improving the internal deduplication rate of a single large data chunk.

We implemented a complete prototype of the FHRD system on the open-source deduplication framework Destor and conducted comprehensive experimental evaluations. The results show that under various mixed workloads, FHRD achieves a significant 20%--30% improvement in deduplication rate compared to traditional methods, with its advantages becoming more pronounced as the proportion of model data and block sizes increase. At the same time, the system's overall overhead is on the same order of magnitude as the baselines, reflecting the efficiency of its lightweight design.

In summary, this research not only accurately diagnoses the bottlenecks faced by traditional deduplication in the AI era but also innovatively proposes and validates a complete solution centered on fine-grained redundancy recognition and adaptive processing, providing strong technical support for addressing current and future data reduction challenges in cloud storage environments.

\end{abstract}
