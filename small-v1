\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix2019_v3}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{url}
\usepackage{float}
\usepackage{geometry}


% Mock bicaption to standard caption
\providecommand{\bicaption}[2]{\caption{#2}}
% Mock tnote
\providecommand{\tnote}[1]{(#1)}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf FHRD: A Fine-grained Hybrid Redundancy Data Reduction System for Mixed Workloads}

%for single author (just remove % characters)
\author{
{\rm Liu Zhuo}\\
Shanghai Jiao Tong University
}

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
With the rapid development of cloud computing and artificial intelligence, cloud storage systems are facing the challenge of explosive data growth. To reduce storage costs, data reduction technologies (such as deduplication and compression) are widely used. However, modern cloud storage workloads exhibit new trends of "large blocks" and "modeling": to improve I/O performance, systems tend to use larger data blocks, making it difficult for traditional block-level deduplication to identify local redundancy within blocks; at the same time, the proportion of AI model data in storage has surged, and its unique floating-point numerical redundancy cannot be effectively eliminated by traditional deduplication or general compression algorithms. Existing single optimization solutions struggle to address this mixed workload simultaneously and may even interfere with each other.

To address these issues, this paper proposes a data reduction system with fine-grained redundancy identification capabilities for cloud storage workloads containing model data—FHRD (Fine-grained Hybrid Redundancy Deduplication). This system builds on the traditional data reduction pipeline by introducing sub-block level redundancy identification and deduplication, as well as model data identification, separation, and encoding compression capabilities.
First, to tackle the local continuous redundancy within large data blocks, a fine-grained sub-block deduplication module is designed, which efficiently removes intra-block redundancy through methods such as exponential rounding bidirectional sub-block fixed-length segmentation.
Second, to address the model data that interferes with the deduplication process in mixed workloads, a byte-granularity grouped sampling entropy analysis method is proposed in the model data separation module, which can accurately and quickly identify model data blocks mixed in the workload and separate them from the conventional deduplication process.
Finally, for the separated model data, a byte-grouped compression method based on entropy analysis conclusions is designed in the model encoding compression module, which reorganizes and arranges floating-point numbers to transform incompressible numerical redundancy into compressible forms, thereby significantly improving the compression ratio.

Experimental results show that under mixed workloads containing model data and traditional data, FHRD achieves an average data reduction rate improvement of 21.7\% compared to traditional "variable-length chunking + block-level deduplication + general compression" solutions such as Destor. As the trends of large blocks and mixed workloads continue to develop, the advantages of this system become increasingly significant, with a maximum data reduction rate improvement of 38.4\%.
\end{abstract}

%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------
The maturity and ubiquity of cloud computing technology have significantly driven the development of the modern information technology industry. As one of its foundation stones, cloud storage systems are tasked with providing reliable and scalable data storage capabilities for massive global users. To effectively cope with the cost pressure brought by large-scale data, \textbf{Data Reduction} technology has become an indispensable core technology in cloud storage systems.

However, in recent years, the efficiency of cloud storage data reduction has encountered a bottleneck, which is closely related to the changes in the workloads it carries.
First, files in cloud storage workloads are becoming larger. To pursue high I/O throughput, modern storage systems tend to adopt larger data block sizes, which leads to a significant decline in the effectiveness of traditional deduplication technologies. As the data block size increases, the probability of finding two completely identical data blocks decreases dramatically. Although these large blocks often contain a large number of local duplicate fragments, traditional coarse-grained deduplication based on the entire block is difficult to detect such "local continuous redundancy" within the block.

Second, with the breakthrough progress of artificial intelligence technology, more and more model data are mixed into cloud storage workloads. Model files are composed of hundreds of millions of independent floating-point tensors, and the data content presents a high degree of randomness and uniqueness. When the system splits different models into data blocks, almost every block will have a unique fingerprint, which makes the deduplication mechanism based on global fingerprint matching completely invalid. At the same time, general compression algorithms are also difficult to capture the high-order "numerical redundancy" inside floating-point numbers.

In summary, these two workload trends jointly shape a mixed workload where "model data and large block data coexist". Based on this, this study proposes and builds a fine-grained redundancy identification data reduction system named FHRD (Fine-grained Hybrid Redundancy Deduplication). This system first adds the ability to identify intra-block continuous redundancy on basis of the traditional deduplication pipeline. Second, the framework incorporates a lightweight model data identification and separation module. Finally, for the separated model data, the framework adopts a dedicated grouped encoding method to compress its numerical redundancy.

%-------------------------------------------------------------------------------
\section{Background and Motivation}
%-------------------------------------------------------------------------------
\subsection{Problem Analysis}
Traditional data reduction systems, which rely on coarse-grained block-level deduplication and general-purpose compression, are becoming increasingly inefficient for modern cloud storage workloads. This inefficiency stems primarily from two emerging trends: increasing block sizes and the proliferation of AI model data.

First, as storage systems adopt larger block sizes to maximize throughput, the effectiveness of traditional deduplication declines. Large blocks often contain significant local repetitive content, but even a single byte difference prevents the entire block from determining a match. As shown in Figure~\ref{fig:block_size_reduction}, our experiments with Linux kernel source tarballs demonstrate that the data reduction rate of a standard Destor+Zstd setup drops noticeably as the average block size increases.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/0302系统数据缩减率随数据块平均大小变化情况.pdf}
    \bicaption{Deduplication rate variation with average data block size}{Deduplication rate variation with average data block size}
    \label{fig:block_size_reduction}
\end{figure}

Second, the influx of model data (e.g., checkpoints) further exacerbates this issue. Model files consist largely of floating-point tensors with high variability, rendering fingerprint-based matching ineffective. As Figure~\ref{fig:mixed_workload_reduction} illustrates, in a mixed workload utilizing Transformer models, the data reduction rate plummets as the proportion of model data rises, approaching 1 (no reduction) when the workload is entirely model data.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/0301系统数据缩减率随模型数据比例变化情况.pdf}
    \bicaption{Deduplication rate variation with model data proportion}{Deduplication rate variation with model data proportion}
    \label{fig:mixed_workload_reduction}
\end{figure}

\subsection{Analysis of Fine-grained Redundancy}
A deeper analysis reveals that while traditional methods fail, significant \textit{fine-grained redundancy} remains unexploited in these workloads:
\begin{itemize}
    \item \textbf{Intra-block local redundancy}: In large non-model data blocks, although the global fingerprints differ, substantial contiguous sections often remain identical.
    \item \textbf{Model numerical redundancy}: In model data, floating-point numbers (BF16/FP32) exhibit high similarity in their exponent bits, while mantissa bits vary. Entropy analysis of Transformer models confirms that the exponent parts have much lower entropy than the mantissas, indicating a potential for compression if processed correctly.
\end{itemize}

\subsection{Design Goals}
To address these challenges and exploit the identified fine-grained redundancy, FHRD aims to achieve three key design goals:
\begin{enumerate}
    \item \textbf{Fine-grained Deduplication}: The system must identify and remove local repetitions within large data blocks, effectively handling the "large block" trend.
    \item \textbf{Model Data Identification}: The system needs a low-overhead method to distinguish model data from traditional data, enabling specialized processing paths.
    \item \textbf{Model-Specific Compression}: For identified model data, the system should employ encoding strategies that transform numerical redundancy into a compressible format.
\end{enumerate}


%-------------------------------------------------------------------------------
\section{System Design and Optimization}
%-------------------------------------------------------------------------------

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/0304细粒度冗余识别的数据缩减系统总体架构.drawio.pdf}
    \bicaption{Architecture of Fine-grained Hybrid Redundancy Deduplication System}{Architecture of Fine-grained Hybrid Redundancy Deduplication System}
    \label{fig:FHRD_architecture}
\end{figure}

\subsection{System Overview}
The FHRD system proposed in this paper, shown in Figure~\ref{fig:FHRD_architecture}, integrates multiple new modules based on the traditional block-level deduplication flow.



The workflow begins with the \textbf{Data Type Separation Module}, which analyzes incoming data blocks to distinguish between model data and non-model data. Non-model data is directed to the \textbf{Fine-grained Sub-block Deduplication Module}, while model data is sent to the \textbf{Model Encoding Compression Module}.

\subsection{Fine-grained Sub-block Deduplication for Non-model Data}
For data blocks identified as non-model data, the system routes them to the Non-model Data Processing Module (Figure~\ref{fig:non_model_processing}). This module is designed to identify and remove fine-grained sub-block redundancy that traditional deduplication misses. Although large blocks often contain local repetitive content, effectively utilizing this redundancy faces several challenges in partitioning strategy.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/0308非模型数据处理模块.drawio.pdf}
    \bicaption{Non-model Data Processing Module Architecture}{Non-model Data Processing Module Architecture}
    \label{fig:non_model_processing}
\end{figure}

\textbf{Challenge 1: Boundary Offset.} Traditional fixed-length chunking suffers from the boundary offset problem. As shown in Figure~\ref{fig:boundary_offset}, traditional data (like text or code) often has small modifications. A simple insertion or deletion shifts the boundaries of all subsequent blocks (yellow parts), preventing deduplication even if the content remains largely consistent.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0405定长切块的边界偏移问题.drawio.pdf}
    \bicaption{Boundary offset problem in fixed-length chunking}{Boundary offset problem in fixed-length chunking}
    \label{fig:boundary_offset}
\end{figure}

Content-Defined Chunking (CDC) (variable-length chunking) solves this problem by determining boundaries based on content hash. As Figure~\ref{fig:variable_length_chunking} illustrates, boundaries shift adaptively, allowing the majority of the content to be correctly identified as duplicate blocks.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0406变长切块解决边界偏移问题.drawio.pdf}
    \bicaption{Variable-length chunking solves the boundary offset problem}{Variable-length chunking solves the boundary offset problem}
    \label{fig:variable_length_chunking}
\end{figure}

\textbf{Challenge 2: Inconsistent Sub-block Division.} However, CDC results in variable block sizes, which complicates sub-block partitioning using fixed number of sub-blocks. If we simply divide variable-length blocks into a fixed number of sub-blocks (e.g., 3 parts), the resulting sub-blocks will have different sizes, as shown in Figure~\ref{fig:fixed_length_chunking}. This misalignment means that even if the red parts are redundant, they cannot be matched because their boundaries do not align.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0407固定份数为 3 的子块划分.drawio.pdf}
    \bicaption{Sub-block division into 3 fixed parts}{Sub-block division into 3 fixed parts}
    \label{fig:fixed_length_chunking}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0408指数取整的双向子块定长切分方法.drawio.pdf}
    \bicaption{Exponential rounding bidirectional sub-block fixed-length segmentation}{Exponential rounding bidirectional sub-block fixed-length segmentation}
    \label{fig:exponential_rounding_chunking}
\end{figure}

To address these issues, we propose the \textbf{Exponential Rounding Bidirectional Sub-block Fixed-length Segmentation} method.
First, the system calculates a sub-block length that is the power of 2 closest to 1/10 of the variable-length data block size. This ensures sub-blocks are appropriately scaled to the parent block and maintain a consistent fixed size across similar blocks.
Then, the system performs fixed-length chunking from both the start and end of the block towards the center.
As shown in Figure~\ref{fig:exponential_rounding_chunking}, this bidirectional approach ensures that even if the middle of the block changes, sub-blocks at both ends remain consistent (Green parts: 00, 11, 33) and can be matched, effectively mitigating boundary shift impacts within the block.



\subsection{Model Data Separation Module}
This module aims to accurately identify model data blocks. Since reliance on file extensions is unreliable, we analyze content characteristics.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0402模型浮点数的表示形式.drawio.pdf}
    \bicaption{Representation of model floating-point numbers}{Representation of model floating-point numbers}
    \label{fig:float_representation}
\end{figure}

\textbf{Principle: Entropy Distribution in Floating-point Numbers.} 
The core distinction of model data lies in its representation. Deep learning models consist of massive arrays of floating-point numbers (FP32, BF16, FP16). As shown in Figure~\ref{fig:float_representation}, a floating-point number is composed of a sign bit, exponent bits, and mantissa bits. In a trained model, the weights within a layer usually share a similar dynamic range, meaning their \textit{exponent bits} are highly similar (low entropy).



To quantitatively verify this, we analyzed the entropy of each bit of BF16 floating-point numbers in Transformer model files. As shown in Figure~\ref{fig:bit_entropy}, the entropy of the exponent part (bits 9\textasciitilde14) is significantly lower than that of the mantissa part. This confirms that while the mantissa bits are highly random (high entropy), the exponent bits contain a large amount of redundancy that traditional methods fail to exploit.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0303模型文件中BF16浮点数各bit位熵值.pdf}
    \bicaption{Bit entropy values of BF16 floating-point numbers in model files}{Bit entropy values of BF16 floating-point numbers in model files}
    \label{fig:bit_entropy}
\end{figure}

Based on this principle, the architecture of the Model Data Separation Module is illustrated in Figure~\ref{fig:model_separation_module}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/0306数据类型鉴别模块.drawio.pdf}
    \bicaption{Model Data Separation Module Architecture}{Model Data Separation Module Architecture}
    \label{fig:model_separation_module}
\end{figure}

\textbf{Challenge.} To detect this pattern, one might check the entropy of specific bits. However, operating at the bit granularity is computationally expensive due to complex bitwise shifting and masking (Figure~\ref{fig:bitwise_complexity}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0401比特粒度实现的复杂性.drawio.pdf}
    \bicaption{Complexity of bit-level implementation}{Complexity of bit-level implementation}
    \label{fig:bitwise_complexity}
\end{figure}

\textbf{Optimization: Byte-granularity Grouped Sampling.} 
To avoid bit-level complexity, we exploit the byte alignment of floating-point formats. As Figure~\ref{fig:float_representation} shows, although exponent bits cross byte boundaries, the "most significant" parts of the exponent often dominate specific bytes.
We propose a \textbf{Byte-granularity Grouped Sampling Entropy Analysis} method. Instead of parsing floats, the system simply treats the data block as a byte stream. It samples bytes at fixed offsets (e.g., stride of 4) to form 4 "byte groups". 
By calculating the entropy of each group, we can identify specific "low-entropy byte groups" (Figure~\ref{fig:bytewise_entropy}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0403字节粒度的分组抽样熵值分析.drawio.pdf}
    \bicaption{Byte-wise grouped sampling entropy analysis}{Byte-wise grouped sampling entropy analysis}
    \label{fig:bytewise_entropy}
\end{figure}

\begin{itemize}
    \item If one low-entropy byte group differs significantly from three high-entropy groups, it indicates the pattern of FP32 (where the exponent concentrates in one byte).
    \item If low-entropy occurs every 2 bytes, it indicates BF16/FP16.
\end{itemize}
This method transforms complex pattern recognition into a fast, vectorizable byte counting task.



\subsection{Model Encoding Compression Module}
The goal of the model data processing module is to identify and concentrate the numerically similar floating-point exponent bits in the model data block, so as to use general compression algorithms for data reduction.

\textbf{Principle: Data Reorganization for Locality.} 
Typical general-purpose compression algorithms (e.g., LZ4, Zstd) are designed to compress text or binary streams by finding \textit{continuous local redundancy}. They use valid windows to match repeated byte sequences. However, in raw model data, the "redundant" bytes (exponent parts) are not continuous; they are separated by "random" bytes (mantissa parts) every few bytes. 
This "stride" pattern breaks the locality required by sliding-window compressors, causing them to fail to match the exponents and wasting effort on the random mantissas.
To fix this, we must reorganize the data to make redundant parts \textit{spatially adjacent}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/0307模型数据处理模块.drawio.pdf}
    \bicaption{Model Data Processing Module Architecture}{Model Data Processing Module Architecture}
    \label{fig:model_processing_module}
\end{figure}

\textbf{Optimization: Byte Grouped Compression Based on Entropy Analysis.} 
We propose a \textbf{Byte Grouped Compression Method} as shown in Figure~\ref{fig:model_processing_module}. 
Crucially, we do not blindly reorganize all bytes. 
Since the separation module (Section 3.3) has already calculated the entropy distribution and identified which byte group contains the low-entropy exponents, we reuse this conclusion.
As shown in Figure~\ref{fig:byte_group_compression}, the system extracts \textit{only} the identified low-entropy byte groups (e.g., Group 1 for FP32) and concatenates them into a continuous block. This block, now consisting of a pure stream of similar exponents, is highly compressible by Zstd. The remaining high-entropy groups are stored essentially uncompressed or lightly compressed. This selective approach reduces the heavy compression workload by 50-75\% compared to re-encoding the entire block, significantly optimizing CPU efficiency.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/0404基于熵值分析的字节分组压缩.drawio.pdf}
    \bicaption{Byte-wise grouped compression based on entropy analysis}{Byte-wise grouped compression based on entropy analysis}
    \label{fig:byte_group_compression}
\end{figure}



%-------------------------------------------------------------------------------
\section{Implementation}
%-------------------------------------------------------------------------------
\subsection{Prototype Implementation}
We implemented a prototype of FHRD based on the open-source deduplication system Destor. The core functions, including fine-grained sub-block deduplication, model data identification, and encoding compression, were implemented with approximately 3,000 lines of C++ code. The system also integrates the Zstandard (Zstd) library for general-purpose compression.

\textbf{Data Block Structure.} To accommodate fine-grained metadata, we reconstructed the data block structure. The container format was extended to store sub-block headers and determining flags, allowing the restoration engine to correctly identify and reassemble fine-grained deduplicated blocks and encoded model data.

\subsection{Pipeline Parallelism}
To improve the overall processing performance, we introduce a pipeline parallel processing method in the end-to-end flow. As shown in Figure~\ref{fig:system_dedup_workflow}, each processing stage is handled by different threads in parallel, and data communication between threads is implemented through global queues.
The pipeline includes: \textbf{Read} thread, \textbf{Chunk} thread, \textbf{Hash} thread, \textbf{Dedup} thread (checking multi-level cache), \textbf{Identify} thread (separating model data), \textbf{Sub-block Dedup} module (Fine-hash/Fine-dedup/Diff threads), \textbf{Model Encode} thread, \textbf{Compress} thread, and \textbf{Save} thread. This design maximizes the utilization of multi-core processor resources and hides the latency overhead of fine-grained processing.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/0309系统去重存储工作流.drawio.pdf}
    \bicaption{System deduplication storage workflow}{System deduplication storage workflow}
    \label{fig:system_dedup_workflow}
\end{figure}

\subsection{Multi-level Index Cache}
To cope with the extra index lookup overhead brought by fine-grained deduplication, the system adopts a multi-level index cache strategy, as shown in Figure~\ref{fig:multi_level_index_cache}.
The lowest layer is the Key-Value (KV) index storage in the disk. Above this, we design two levels of memory cache:
\begin{itemize}
    \item \textbf{L1 Cache (Global Cache)}: Uses LRU policy to store the most frequently used data fingerprints mappings.
    \item \textbf{L2 Cache (Local Cache)}: Stores mappings for the currently processing data segment to reduce random access to the underlying storage.
\end{itemize}
Ideally, the system sequentially queries the Local Cache, Temporary Index Cache, and Global Cache during a duplicate block match or similar block lookup.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/0409多级索引缓存下的匹配查找时序.drawio.pdf}
    \bicaption{Matching lookup timing with multi-level index cache}{Matching lookup timing with multi-level index cache}
    \label{fig:multi_level_index_cache}
\end{figure}

%-------------------------------------------------------------------------------
\section{Evaluation}
%-------------------------------------------------------------------------------
\subsection{Experimental Setup}
We use a dual-socket AMD EPYC 7763 server for testing. Comparison schemes include the basic Destor (BASE), incremental compression ODESS, model compression ZIPNN, and BURST.
The workload datasets cover a wide range, including non-model data (such as backup, code, lnx-ker) and model data (such as checkpoints, vgg19).

\subsection{Results}
Experimental results show that FHRD performs excellently in mixed workload scenarios.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/0501各类混合负载下数据缩减率对比.pdf}
    \bicaption{Comparison of Data Reduction Rates under Various Mixed Workloads}{Comparison of Data Reduction Rates under Various Mixed Workloads}
    \label{fig:dedup_rate}
\end{figure}

As shown in Figure~\ref{fig:dedup_rate}, FHRD achieved higher data reduction rates compared to the traditional BASE scheme under various mixed workloads. The average data reduction rate increased by 21.7\%. Especially when the proportion of model data is high, FHRD, relying on the dedicated model processing module, can still maintain a high reduction rate, while the traditional scheme drops sharply.

%-------------------------------------------------------------------------------
\section{Related Work}
%-------------------------------------------------------------------------------
Traditional block-level deduplication technologies (Fixed-Size Chunking and Content-Defined Chunking) are the cornerstone of cloud storage systems, but facing the "large block" and "modeling" trends of modern workloads, their limitations are becoming increasingly prominent.
Incremental compression technologies (such as Odess) can excavate finer-grained redundancy but have huge computational overhead.
Model compression technologies (such as ZipNN) are effective for model data but lack the ability to identify mixed workloads.
FHRD systematically solves the above problems by combining fine-grained deduplication and model identification separation technologies, significantly improving the data reduction rate of mixed workloads while ensuring performance.

%-------------------------------------------------------------------------------
\section{Conclusion}
%-------------------------------------------------------------------------------
Focusing on the challenge of mixed workloads where model data and large block data coexist in modern cloud storage, this paper proposes the fine-grained redundancy identification data reduction system FHRD. By introducing fine-grained sub-block deduplication and model data identification and separation mechanisms, FHRD can effectively identify and remove intra-block local redundancy and model numerical redundancy. Experimental results show that FHRD significantly improves the data reduction rate under mixed workloads and has good system performance, providing an efficient data reduction solution for modern cloud storage systems.

{\small
\bibliographystyle{plain}
\bibliography{refs}
}

%-------------------------------------------------------------------------------
\end{document}
%-------------------------------------------------------------------------------
