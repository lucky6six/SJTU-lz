% !TEX root = ../main.tex

\chapter{相关技术概述}

\section{云存储服务}
云存储服务作为云计算基础设施的核心组成部分，已成为现代企业数据管理的关键支撑。云存储通过虚拟化技术将分布在大量存储设备上的资源统一管理，形成可弹性扩展的存储资源池，为用户提供按需分配、弹性伸缩、高可用且持久化的数据存储能力。与传统本地存储相比，云存储将存储资源池化并通过网络提供服务，实现了资源的解耦与共享，显著提升了资源利用率和系统灵活性。

根据访问接口和数据组织方式的不同，云存储服务主要分为以下几类：

（1）对象存储：对象存储以扁平化的命名空间组织数据，每个对象通过全局唯一的标识符（如 \texttt{Key}）进行访问，而非传统的目录层级结构。典型代表包括 Amazon S3、阿里云 OSS、Google Cloud Storage 等。对象存储适用于存储海量非结构化数据，如图片、视频、备份文件等，具备极高的可扩展性、持久性和成本效益。其通常采用多副本或纠删码技术保证数据的可靠性，并支持 RESTful API 进行访问，便于跨网络集成。

（2）文件存储：文件存储提供类似传统文件系统的树状目录结构，支持标准的文件操作接口（如 POSIX），适用于需要共享访问和复杂目录结构的场景。典型系统包括 Google File System 及其后继者 Colossus，以及云服务商提供的 Amazon EFS、Azure Files 等。这类系统通过将文件分块并分布到多个节点上，实现了高吞吐和容错能力，广泛用于大数据分析和机器学习训练等数据密集型应用。

（3）块存储：块存储将存储空间抽象为裸设备块，通过网络挂载到计算节点上，可作为虚拟机的系统盘或数据盘使用。典型产品包括 Amazon EBS、Azure Disk、Ceph RBD 等。块存储提供低延迟、高 \texttt{IOPS} 的访问能力，适用于数据库、文件系统等需要直接块设备接口的应用场景。

从技术架构角度看，典型的云存储系统如图~\ref{fig:cloud_storage_architecture} 所示，采用分层设计，自上而下分别为应用接口层、存储服务层和分布式存储基础层。


最上层是应用接口层，它通过标准的 RESTful API 或特定协议的客户端向最终用户对外提供服务，使得应用程序能够以简单、统一的方式访问底层的复杂存储资源。

中间层是核心的存储服务层，它负责将底层存储资源抽象并封装成不同类型的服务，包括对象存储、文件存储和块存储。正是在这一层，云存储服务商集成了关键的数据缩减技术，其中最典型的是重复数据删除与数据压缩。在数据持久化到底层之前，这些技术能够有效消除冗余，显著降低存储成本并提升网络传输效率。

最底层是分布式存储基础层，由如 HDFS \cite{borthakur2008hdfs}、Ceph \cite{weil2006ceph} 等系统构成，负责数据的物理存储、多副本或纠删码冗余、故障恢复以及跨节点的资源调度等基础管理职能。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/01.pdf}
    \bicaption{云存储服务系统架构}{System architecture for cloud storage services}
    \label{fig:cloud_storage_architecture}
\end{figure}

这种清晰的分层架构，使得云存储系统能够灵活地同时满足高性能计算、大数据分析、备份归档等多种应用场景的需求。

在成本构成方面，存储硬件采购和维护成本占据了云服务商运营成本的较大比重。为了在激烈的市场竞争中保持价格优势，云服务商必须采用高效的数据缩减技术。数据缩减效果直接影响到云存储服务的利润率，这也是各大云服务商持续投入重金研发数据缩减技术的根本动力。Amazon S3 智能分层、阿里云高效云盘、腾讯云极速型云硬盘等产品都集成了先进的数据缩减技术，在保证性能的同时显著降低了存储成本。
\section{传统重复数据删除系统}
% \subsection{概述}
传统重复数据删除系统如图~\ref{fig:dedup_architecture} 所示，通常采用“先去重，后压缩”的两阶段处理流水线，其核心目标是通过消除冗余数据来最大化存储空间节省。这一经典架构包含四个关键步骤，构成了一个完整的数据缩减工作流。

系统处理流程始于数据切块阶段。原始数据流首先被分割为更小的管理单元，即数据块。接着进入指纹计算阶段，系统为每个数据块通过密码学哈希函数（如 \texttt{SHA-1} 或 \texttt{SHA-256}）生成唯一的数字指纹。这些指纹作为数据块的唯一标识符，构成了去重判断的基础。
匹配去重阶段是系统的核心。系统维护一个全局的指纹索引，通过查询比对每个新数据块的指纹是否已存在。如果发现匹配的指纹，则判定为重复数据块，系统仅需存储指向已有物理块的元数据指针；如果未发现匹配，则将该数据块判定为唯一块，进入下一处理阶段。
最后，在数据压缩阶段，所有唯一数据块会经过通用压缩算法的进一步处理，消除块内部的统计冗余，实现存储空间的二次优化。

这种“切块-指纹-匹配去重-压缩”的设计 \cite{wallace2012characteristics,meyer2012study}，使得传统重复数据删除系统能够在保证数据完整性的同时，显著提升存储效率。然而，随着数据特征的演变，这一经典架构在面对新型工作负载时逐渐暴露出其局限性，特别是对 AI 模型数据中的数值冗余和大块数据中的局部连续重复缺乏有效的处理能力。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/传统重复数据删除系统.drawio.pdf}
    \bicaption{传统重复数据删除系统架构}{Architecture of traditional deduplication system}
    \label{fig:dedup_architecture}
\end{figure}
% \subsection{切块}

固定大小分块是最基础且直观的数据分块方法。该方法将数据流视为连续的字节序列，按照预设的固定长度（如 4KB、8KB 或 64KB）进行均匀切分，生成一系列大小相同的数据块 \cite{quinlan2002venti}。
固定大小分块实现简单，计算开销小。由于分块边界仅由偏移量决定，无需复杂计算，故在处理速度上具有显著优势。在数据内容相对静态的场景（如完整虚拟机镜像的初始备份、大规模软件分发等）中，该技术能提供稳定可靠的去重效果。此外，固定大小的数据块便于存储管理与资源分配，降低系统设计复杂性。

然而，该技术存在边界偏移问题。当数据发生插入或删除操作时，即使只是单个字节的变动，也会导致后续所有数据块的边界发生偏移。例如，在一个文本文件中插入一行内容，会导致从插入点到文件结尾的所有数据块内容发生改变。这种现象被称为“边界偏移效应”，它使得原本相同的内容在新数据流中因位置变化而被识别为不同的数据块，这种连锁反应使得系统无法识别出文档中实际存在的大量重复内容，严重削弱了去重效果。

内容定义切块技术（Content-Defined Chunking，CDC），也被称为变长切块，是解决固定分块边界偏移问题的有效方法。其核心思想是利用数据内容自身的特征来确定分块边界，使得分块结果对数据的插入和删除操作具有鲁棒性 \cite{muthitacharoen2001low}。
Rabin 算法 \cite{rabin1981fingerprinting} 是内容定义切块技术中最具代表性的实现方式。该算法使用滑动窗口在数据流上滑动，为窗口内的数据计算一个哈希值（即 Rabin 指纹）。当该哈希值的低位与预定义的阈值匹配时，即认为找到了一个切分点。通过调整参数，可以控制块大小的期望值和方差。Rabin 算法的核心优势在于其滚动哈希特性，当窗口滑动时，新的哈希值可以通过简单的计算从旧哈希值推导得出，而不需要重新计算整个窗口的哈希，这使得算法具有较高的效率。
在实际应用中，内容定义切块技术算法需要解决块大小分布控制的问题。理想的块大小应该在一定范围内波动，既要避免产生过多的小块导致元数据膨胀，也要防止出现过大的块影响去重效果。常见的做法是设置最小块大小和最大块大小的限制，当窗口滑动超过最大块大小时强制进行切分。

% \subsection{哈希指纹}
哈希指纹是重复数据删除系统中用于数据块唯一性标识的关键技术。相同内容的数据块应该生成相同的指纹，而不同内容的数据块则应该生成不同的指纹。同时，一个理想的哈希函数应该具有较高的计算效率。
在早期的去重系统中，\texttt{MD5} 和 \texttt{SHA-1} 等加密哈希函数被广泛使用 \cite{srinivasan2012idedup,xia2011silo}。这些函数具有很好的抗碰撞性，但计算开销较大。随着云服务安全要求的逐渐提升，\texttt{SHA-256} 等更安全的哈希算法逐渐成为主流 \cite{drago2012inside}，但其计算成本也相应增加 \cite{rachmawati2018comparative}。

哈希索引的管理是另一个关键技术挑战。随着数据量的增长，指纹索引可能达到 TB 级别，无法完全存放在内存中。学术界提出了多种解决方案，包括布隆过滤器 \cite{bloom1970space}、级联布隆过滤器、局部性容器管理 \cite{zhu2008avoiding} 等数据结构，基于容器的磁盘数据管理以及基于磁盘的索引分区策略。这些技术能够在可接受的误判率下，大幅降低索引对内存的需求。
% \subsection{压缩算法}

在重复数据删除之后，压缩算法作为数据缩减流水线的第二道工序，负责进一步消除唯一数据块内部的统计冗余。
LZ 系列算法基于字典编码原理，通过寻找并替换数据中重复出现的字符串来实现压缩。LZ4 算法特别注重压缩和解压速度，采用简化的哈希链机制和字节对齐的编码格式，实现了极高的吞吐量。在典型的 x86 平台上，LZ4 的压缩速度可达数百 MB/s，解压速度接近 GB/s，使其非常适合实时数据处理场景。
Zstandard 算法在 LZ77 \cite{ziv2003universal} 的基础上引入了有限状态熵编码技术。FSE 是 ANS 算法的一种高效实现，能够以接近香农极限的效率进行熵编码，同时保持硬件友好的特性。Zstandard 还提供了从负值到 22 的多级压缩预设，用户可根据具体需求在速度与压缩率之间进行调节。在默认级别下，Zstandard 通常能达到比 Zlib 更好的压缩率，同时保持更快的速度。
在选择压缩算法时，需要综合考虑数据类型、性能要求和硬件特性。文本类数据通常能够获得较好的压缩效果，而已经压缩过的数据（如图片、视频）往往压缩空间有限。在一些对延迟敏感的场景中，甚至会选择不压缩以避免额外的 CPU 开销。

\section{细粒度去重技术}
传统重复数据删除技术基于精确匹配机制，仅能识别和消除完全一致的数据块。然而，在实际应用场景中，大量数据块之间存在高度相似性但并不完全相同。例如，虚拟机镜像的连续版本、文档的修订历史、日志文件的轮转备份等，这些数据在整体结构上保持高度一致，但在局部存在细微差异。传统去重技术对此类数据的处理效果极为有限，无法有效挖掘其中的冗余空间。

增量压缩技术正是为解决这一问题而提出的。它旨在识别和利用数据块之间的相似性，通过存储相似块之间的差异而非完整数据，实现对相似但不相同数据的高效压缩。这种技术突破了传统去重必须精确匹配的限制，为数据缩减开辟了新的可能性。

典型的增量压缩系统包含三个关键环节：特征提取、相似匹配和差分计算：

\begin{itemize}
    \item 特征提取：为每个数据块计算一个或多个“特征”。该特征是数据块的抽象表示，其核心要求是：语义相似的块，其特征也应相似。常用的方法包括基于内容定义的滚动哈希（如 Rabin 指纹）和统计抽样等。
    \item 相似匹配：系统维护一个特征索引。当处理新数据块时，通过快速比对其特征与索引中已有特征的相似度，来寻找一个或多个最相似的候选参考块。
    \item 差分计算：在找到参考块后，通过差分算法计算目标块与参考块之间的精确差异。主流的差分算法通过寻找最长的公共子序列，并记录下插入、删除和替换操作，来生成一个紧凑的差异补丁文件。
\end{itemize}


增量压缩技术在多个重要场景中展现出显著价值。在备份服务中，它可以有效处理连续版本的数据备份 \cite{xia2014combining}。在分布式存储系统中，增量压缩大幅降低了节点间的数据同步开销，当某个节点需要更新数据时，系统只需传输差异部分而非完整文件 \cite{shilane2012wan}。在容器镜像分发场景中，增量压缩实现了分层镜像的高效存储，降低了相同基础镜像的派生镜像的存储开销。

但增量压缩在实际应用中落地仍面临多个挑战。首先是计算复杂度问题。与传统块级匹配去重相比，增量压缩需要额外的特征提取、相似匹配和差分计算步骤，这些操作都会增加 CPU 和内存开销。在数据规模达到 TB 级别时，这些开销可能变得不可忽视。其次是系统复杂性的增加。增量压缩需要维护额外的特征索引和相似度关系，这增加了系统的设计复杂度和维护成本。在分布式环境中，如何保持特征索引的一致性和及时更新也是一个技术挑战。尤其在如今云存储的混合工作负载环境下，增量压缩可能产生负面效果。当处理 AI 模型数据时，由于模型参数的随机分布特性，增量压缩难以找到有效的相似关系，反而会带来不必要的计算开销。这种局限性促使我们需要更智能的数据类型识别机制，这正是本研究的重要创新点所在。


除了增量压缩，研究者还提出了多种细粒度去重技术来应对不同的应用场景。
Burst \cite{pengburst} 专注于解决大块数据内部的局部重复问题。该技术将 MB 级的数据块进一步细分为更小的片段，通过滑动窗口和哈希匹配来识别重复的字节序列。与传统去重不同，Burst 不需要全局的指纹索引，而是基于局部性原理在数据块内部进行冗余消除。这种方法特别适合处理日志文件、文档集合等包含大量局部重复的数据。
基于语义的去重技术是另一个研究方向。在数据库存储中，Apache ORC 和 Parquet 等列式存储格式通过分析数据的语义特征，在列级别进行重复数据删除。
在文件系统层面，SDFS 实现了基于文件内容的可变粒度去重，能够根据文件类型自动调整处理策略。

\section{模型压缩技术}

随着深度学习模型规模的不断扩大，模型压缩技术成为提升模型部署效率和降低存储成本的关键手段。

网络剪枝技术通过移除神经网络中的冗余参数来实现模型压缩，主要分为结构化剪枝和非结构化剪枝两大类。
非结构化剪枝着眼于移除单个不重要的权重。早期的工作基于权重绝对值的大小进行剪枝，虽然简单有效，但可能破坏网络的整体结构。近年来，基于敏感度的剪枝方法逐渐成为主流，这些方法通过分析权重对最终损失函数的影响来确定剪枝优先级。彩票假说理论进一步发现，密集的随机网络中存在着稀疏的子网络，在独立训练时能够达到原网络的性能。
结构化剪枝在更高粒度上进行操作，移除整个神经元、通道或者层。这种方法的好处是能够保持规整的网络结构，便于在通用硬件上高效推理。通道剪枝通常基于 L1 范数或者 BN 层缩放因子来选择要移除的通道。层剪枝则通过分析各层对最终输出的贡献度，移除冗余的层结构。


模型量化技术通过降低神经网络中权重和激活值的数值精度来实现模型压缩和加速。根据量化粒度的不同，可以分为权重量化、激活量化和梯度量化。
训练后量化不需要重新训练模型，直接对训练好的 FP32 模型进行量化。对称量化将权重映射到 [-127,127] 的范围，而非对称量化则使用完整的 [-128,127] 范围。为了处理激活值的动态范围，动态量化方案会在线统计激活值的分布特性。PQAT 在训练过程中模拟量化效果，通过 STE 反向传播梯度，使模型能够适应低精度表示。
混合精度量化根据各层对量化误差的敏感度分配不同的精度。卷积层通常对量化更敏感，需要保持较高精度，而全连接层可以承受更强的量化。NAS 技术可以自动搜索各层的最优精度配置，在保持模型精度的同时最大化压缩效果。

模型的剪枝与量化依赖模型的训练过程，且是有损压缩技术，对于云存储场景而言，用户上传的往往是预训练好的模型文件，且不接受有损存储，无法直接应用这些技术。
而 Gzip 等通用无损压缩算法在处理模型文件时效果有限，因为模型文件中存在大量的数值冗余，这些冗余无法通过传统的统计压缩方法有效去除。

针对这一问题，研究者提出了多种专用的模型压缩技术。如 ZipNN \cite{hershcovitch2025zipnn} 通过重排模型参数中浮点数的符号位、指数位和尾数位，将相同性质的位集中在一起，再利用通用压缩器进行压缩，从而实现对单个模型的压缩。与之相比，ZipLLM \cite{wang2026zipllm} 专为大规模模型库设计，通过 BitX 算法将微调模型与基础模型进行逐参数比特位的 XOR 操作，生成一个稀疏的、富含零值的差异文件，再使用通用压缩器进行高效压缩，从而实现对模型家族内的冗余消除。FM-Delta \cite{ning2024fm} 则针对模型更新场景，通过计算新旧模型参数的增量差异，生成一个紧凑的差异文件，实现对模型版本间冗余的高效去除。

然而，这些方法的显著局限性在于其强烈的“模型感知”特性。它们深度依赖 LLM 数据的特定结构，如规整的张量布局、一致的浮点数格式（BF16/FP32）以及模型家族内参数的高度相似性。若将其直接应用于传统的、无此结构的用户数据（如文档、图片），由于无法找到可重排的浮点字段或进行有效的 XOR 差分，压缩过程反而会因添加了额外的处理步骤和元数据而产生“负优化”，即压缩后体积可能大于直接使用通用压缩器。因此，这些技术是高度专业化的，无法作为一种通用数据压缩算法直接集成到处理混合数据类型的云存储去重工作流中。
\section{本章小结}
本章系统回顾了云存储服务的基本架构及其在现代数据管理中的重要性，重点分析了传统重复数据删除系统的工作原理及其局限性。通过对切块、哈希指纹和压缩算法等关键技术的深入探讨，我们揭示了传统去重方法在处理新型数据特征时面临的挑战。随后，介绍了细粒度去重技术，如增量压缩技术在挖掘数据相似性方面的优势及其应用场景，并说明了其性能上的和面向模型数据上的局限性。此外，我们还概述了模型压缩技术的发展现状，强调了其在云存储环境中的适用性限制。整体而言，本章为后续章节中提出的新型数据缩减方法奠定了坚实的理论基础，明确了研究方向和技术需求。
