% !TEX root = ../main.tex

\chapter{相关技术概述}

\section{传统数据缩减系统}
传统数据缩减系统，其架构如图~\ref{fig:dedup_architecture}所示，通常采用“先去重，后压缩”的两阶段处理流水线，核心目标在于通过消除数据冗余以最大化存储空间节省。
其中去重（通常为块级去重）由数据分块、指纹计算、匹配去重三个关键步骤构成~\cite{RJXB201005007}，与数据压缩共同组成一个完整的数据缩减工作流。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0201传统数据缩减系统.drawio.pdf}
    \bicaption{传统数据缩减系统架构}{Architecture of traditional data reduction system}
    \label{fig:dedup_architecture}
\end{figure}

系统处理流程始于\textbf{数据分块}（Chunking）阶段。在此阶段，原始数据流被分割成一系列较小且易于管理的单元，即数据块（Chunk）。

随后进入\textbf{指纹计算}（Fingerprinting）阶段。系统利用密码学哈希函数（如 \texttt{SHA-1} 或 \texttt{SHA-256}），依据数据块内容为每个数据块生成一个唯一的数字指纹（Fingerprint）。
作为后续去重决策的依据。

\textbf{匹配去重}（Duplicate Check）是系统的核心环节。系统维护一个全局的指纹索引库（Index），用于存储所有已处理过的数据块的指纹。当新数据块的指纹生成后，系统会查询索引库。若发现匹配的指纹，则表明该数据块是重复的，系统仅需存储一个指向已有物理数据块的元数据指针即可，无需再次存储相同的数据。反之，若未发现匹配，则该数据块被判定为非重复块/唯一块（Unique Chunk），并被送入下一处理阶段。

最后，在\textbf{数据压缩}（Compression）阶段，所有唯一数据块会经过通用压缩算法（如 Gzip、LZ4、Zstd 等）的进一步处理。这一步骤旨在消除单个数据块内部的统计冗余，从而实现存储空间的二次优化。

\subsection{数据分块}

\textbf{固定大小分块（Fixed-Size Chunking, FSC）}是最基础、最直观的数据分块方法。该方法将输入的数据流视为一个连续的字节序列，并按照预设的固定长度（如 4KB、8KB 或 64KB）进行均匀切分，从而生成一系列大小完全相同的数据块。
FSC 的实现简单，计算开销低。由于分块边界仅由数据在流中的偏移量唯一确定，无需进行任何复杂的内容分析，因此其处理速度具有显著优势。在数据内容相对静态的场景中，如虚拟机镜像的首次全量备份、大规模软件分发包的归档等，该技术能够提供稳定且可靠的去重效果。此外，大小均一的数据块也简化了存储空间的管理与分配，降低了系统设计的整体复杂性。

然而，FSC 技术存在一个致命的缺陷，即\textbf{边界偏移问题（Boundary Shift Problem）}。当数据流中发生任何插入或删除操作时，哪怕只是单个字节的变动，都会导致该变动点之后的所有数据块边界发生连锁式偏移。例如，在一个文本文件中部插入一行文字，将导致从插入点到文件末尾的所有后续数据块的内容全部改变。这种现象使得原本相同的大段内容，在新生成的数据流中因其所处位置的变化而被识别为全新的、不同的数据块。这种连锁反应严重削弱了去重系统识别重复内容的能力，从而导致去重效率急剧下降。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0202变长切块.png}
    \bicaption{变长分块示意图\cite{xia2016fastcdc}}{Illustration of variable-length chunking}
    \label{fig:variable_length}
\end{figure}

如图~\ref{fig:variable_length}所示，\textbf{内容定义分块（Content-Defined Chunking, CDC）}，又称变长分块，是为解决固定分块的边界偏移问题而设计的关键技术。其核心思想是根据数据内容自身的特征来动态确定分块边界，从而使得分块结果对数据的插入和删除操作具有良好的鲁棒性~\cite{muthitacharoen2001low}。
Rabin 算法~\cite{rabin1981fingerprinting} 是 CDC 技术中最具代表性且应用最广泛的一种实现。该算法采用一个固定大小的滑动窗口（Sliding Window）在数据流上逐字节滑动，并为窗口内的数据高效地计算一个哈希值，即 Rabin 指纹。当该哈希值的某些特定位（通常是低位）与一个预定义的模式（Pattern）匹配时，就将当前窗口的结束位置标记为一个切分点。Rabin 算法的核心优势在于其“滚动哈希”（Rolling Hash）的特性：当窗口向前滑动一个字节时，新的哈希值可以通过一次简单的数学运算从旧的哈希值推导得出，无需重新对整个窗口的数据进行完整计算，这使得 CDC 过程极为高效。


在实际应用中，CDC 算法需要精细地控制生成块的大小分布。理想情况下，块大小应在一个合理的范围内波动，既要避免产生大量过小的块（这会导致元数据急剧膨胀，增加管理开销），也要防止出现少数过大的块（这会降低发现重复数据的概率）。为实现这一目标，通常会设定一个最小块大小（Min-Size）和最大块大小（Max-Size）。只有当滑动窗口超过最小块大小时，才开始检查哈希值；而当窗口滑动距离超过最大块大小时，则无论哈希值是否匹配，都会强制进行一次切分。
   
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0203指纹索引.png}
    \bicaption{哈希指纹与索引管理\cite{xia2020design}}{Hash fingerprint and index management}
    \label{fig:hash_fingerprint_index}
\end{figure}
\subsection{哈希指纹/索引}

哈希指纹是数据缩减系统中最重要的元数据，系统据此进行数据块的匹配去重和索引管理。如图~\ref{fig:hash_fingerprint_index}所示，一方面，哈希指纹能够唯一标识每个数据块，
帮助快速确定待存储数据块是否为重复块；另一方面，通过在文件元数据中加入索引构成，记录文件所含数据块的具体位置，通过指纹的索引管理
确保了系统在海量数据中快速定位和访问相关数据块。

一个理想的哈希函数需要满足以下条件：首先，对于相同内容的数据块，
必须生成完全相同的指纹（确定性）；其次，对于不同内容的数据块，生成相同指纹的概率（即哈希碰撞）必须极低（抗碰撞性）；
最后，哈希计算本身的速度要足够快，以满足高吞吐的数据处理需求。

在早期的去重系统中，\texttt{MD5} 和 \texttt{SHA-1} 等加密哈希函数因其良好的抗碰撞性
而被广泛采用~\cite{srinivasan2012idedup,xia2011silo}。然而，随着计算能力的提升和安全标准的演进
，这些算法的安全性受到挑战。因此，现代云存储系统逐渐转向使用如 \texttt{SHA-256} 等更为安全
的哈希算法~\cite{drago2012inside}，尽管这会带来一定的计算成本增加。

\subsection{压缩算法}
在去重流程之后，压缩算法作为数据缩减流水线的最后一道防线，负责进一步消除每个唯一数据块内部的统计冗余。

LZ 系列算法是基于字典编码原理的经典压缩方法，其通过查找并替换数据中重复出现的字符串来实现压缩。其中，LZ4 算法以其极致的速度而著称，它采用简化的哈希链机制和字节对齐的编码格式，实现了极高的处理吞吐量。在典型的 x86 平台上，LZ4 的压缩速度可达数百 MB/s，而解压速度更是接近 GB/s，使其成为实时数据处理、高速缓存等对延迟高度敏感场景的理想选择。

Zstandard（Zstd）算法则代表了速度与压缩率的平衡。它在经典的 LZ77~\cite{ziv2003universal} 算法基础上，创新性地引入了非对称数字系统（Asymmetric Numeral Systems, ANS）的一种高效实现——有限状态熵编码（Finite State Entropy, FSE）。FSE 能够以接近信息论香农极限的效率进行熵编码，同时保持对现代 CPU 硬件友好的特性。Zstandard 提供了从负到正共 22 个压缩级别，允许用户根据具体应用需求，在压缩速度与压缩率之间做出灵活的权衡。在默认级别下，Zstd 通常能提供比传统 Zlib 更高的压缩率，同时保持更快的压缩和解压速度。

在选择压缩算法时，系统需要综合考量数据类型、性能要求及硬件特性。例如，文本、日志等非结构化数据通常具有较高的统计冗余，能够获得良好的压缩效果；而已经过压缩的媒体文件（如 JPEG 图片、MP4 视频）或加密数据，其内容近似于随机噪声，压缩空间极为有限。在某些对 I/O 延迟极度敏感的应用场景中，系统甚至会选择完全禁用压缩，以避免引入额外的 CPU 计算开销。



综上，2.1节中“分块-指纹-匹配去重-压缩”的经典设计~\cite{wallace2012characteristics,meyer2012study}，使得传统数据缩减系统能够在保证数据完整性的前提下，显著提升存储效率。
然而，随着数据形态的不断演变，在处理 AI 模型数据中普遍存在的数值冗余，以及大块数据内部的局部连续重复时，
这一架构在面对新型工作负载时逐渐暴露出其局限性。

\section{增量压缩}



传统去重技术基于精确匹配机制，仅能识别和消除完全一致的数据块，对于存在高度相似性但并不完全相同的数据块处理效果有限。为解决这一问题，更细粒度地挖掘块内冗余，
增量压缩技术应运而生，它通过识别并利用数据块之间的相似性，仅存储它们之间的差异，从而突破了精确块匹配的限制~\cite{1014231504}。

以Odess~\cite{1024587210}为代表的典型增量压缩工作流如图~\ref{fig:incremental_compression}所示,在块级去重后衔接了更细粒度的冗余去除，包含三个关键环节：特征提取、相似匹配和差分计算~\cite{1024581608}：

第一步是\textbf{特征提取}：为每个数据块计算一个或多个能够代表其内容的“特征”,用于相似性匹配。理想的特征应保证语义相似的块具有相似的特征。
    
第二步是\textbf{相似匹配}：通过快速比对新数据块的特征与索引中已有特征的相似度，寻找最相似的候选参考块。
    
第三步是\textbf{差分计算}：在找到参考块后，利用差分算法（如 \texttt{xdelta}）计算目标块与参考块之间的精确差异，生成紧凑的差异补丁文件，而后系统仅存储该较小的差异文件，达到数据缩减的效果。


增量压缩技术在备份服务~\cite{xia2014combining}、分布式存储~\cite{shilane2012wan}和容器镜像分发等场景中展现出显著价值。
然而，其计算复杂度高、系统设计复杂，且在处理 AI 模型等缺乏明显序列的数据时，可能产生不必要的计算开销却得不到数据缩减效果。
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0204odess增量压缩.png}
    \bicaption{增量压缩工作流示意图\cite{xia2023design}}{Illustration of incremental compression workflow}
    \label{fig:incremental_compression}
\end{figure}



\section{细粒度冗余识别}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0205burst.png}
    \bicaption{Burst去重原理示意图\cite{pengburst}}{Illustration of Burst deduplication principle}
    \label{fig:fine_grained_deduplication}
\end{figure}

除增量压缩外，还有研究者关注到大块数据内部的局部重复问题，以另一种方式进行细粒度冗余识别。
例如，Burst~\cite{pengburst}发现备份系统中的大量相似数据块存在“仅有中间部分突发差异”的特点，通过识别相同的文件头或文件尾部，消除大块内部的边缘冗余数据。
如图~\ref{fig:fine_grained_deduplication}所示，Burst 系统在哈希阶段不仅扫描数据块生成块指纹（checksum（badge）），而且会依据头/尾内容生成头/尾指纹（checksum（ba））。
Burst据此在匹配阶段识别出具有相同前缀或后缀的块（bandage）作为参考块，去除大块头部冗余（ba）和尾部冗余（ge），只对中间的差异部分进行存储（nda->d），从而实现更细粒度的冗余消除。
Burst只针对定长分块做了优化，其头尾划分依赖于块大小，且其方法仅针对数据块中的边缘部分，因此尚未充分地利用数据中的连续冗余，但这种将大块切割成更细粒度的组成部分，并提取子块特征的思想值得进一步探索。

\section{模型编码压缩技术}
随着大语言模型规模的急剧扩张，模型压缩技术已成为提升部署效率、降低存储与计算成本的关键。主流技术可分为两大类：

第一类是通过牺牲一部分模型精度来换取更小的模型体积，即缩小模型本身，主要包括\textbf{剪枝}（Pruning）和\textbf{量化}（Quantization）技术。
剪枝技术通过移除神经网络中冗余或不重要的连接和神经元，减少模型参数数量，从而降低存储需求和计算复杂度。
量化技术则通过将高精度的浮点数参数转换为低精度表示（如 8 位整数），显著减少模型的存储空间，同时加速推理过程。
然而，剪枝与量化技术属于\textbf{有损压缩}，且严重依赖模型的训练过程。在云存储场景中，用户上传的通常是预训练好的模型文件，
并且期望无损存储，因此无法应用这些技术。

第二类是在不改变模型内容的前提下，通过编码压缩模型，即缩小存储的空间。
由于通用无损压缩算法（如 Gzip等）在处理模型文件时效果不佳，难以有效处理模型参数中普遍存在的数值冗余，
研究者提出了多种专用的无损模型压缩方法。例如，ZipNN~\cite{hershcovitch2025zipnn}通过重排浮点数结构来提升通用压缩器的效率；
ZipLLM~\cite{wang2026zipllm}则利用微调模型间的高度相似性，通过比特位异或操作生成稀疏的差异文件；
而 FM-Delta~\cite{ning2024fm}专注于模型更新场景，通过计算版本间的增量差异进行压缩。

如图~\ref{fig:zipllm}所示，ZipLLM 系统架构包括模型预处理、差异计算和压缩存储三个主要模块。首先，系统对上传的 LLM 模型进行预处理，
提取代表特征，搜索相似的同类模型。接着，利用参考模型（通常是同类的预训练模型）计算目标模型与参考模型之间的比特级差异（如XOR等编码方法），
生成一个稀疏的差异文件。最后，该差异文件经过高效的无损压缩算法处理后存储，从而实现显著的存储空间节省。这些方法在Huggingface等模型存储库
中展现出优异的压缩效果，但其缩减的冗余来源于同系列微调模型间的不变数据，即多个 LoRA 模型。而在云存储负载中，同一用户空间内通常不会保存多个同系列
模型，因此难以取得理想效果。
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0206模型压缩.png}
    \bicaption{ZipLLM 系统架构示意图\cite{wang2026zipllm}}{Illustration of ZipLLM system architecture}
    \label{fig:zipllm}
\end{figure}


另一方面，如图~\ref{fig:zipnn}所示，ZipNN~\cite{hershcovitch2025zipnn}通过重排浮点数来提升通用压缩器的效率。与 ZipLLM 不同，该方法更加关注模型内部的数值冗余，能够在不依赖于同类模型的情况下，
独立地对单个模型进行压缩，利用的是模型参数中的浮点数数值冗余，这可以为本系统在云存储中实现模型数据压缩提供思路。
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/0207zipnn.png}
    \bicaption{ZipNN 原理示意图\cite{hershcovitch2025zipnn}}{Illustration of ZipNN principle}
    \label{fig:zipnn}
\end{figure}

这些专用方法的局限性在于其与模型特征的高度绑定，它们深度依赖于 LLM 数据的特定结构（如张量布局、浮点数格式等）。
若将它们应用于文档、图片等传统用户数据，不仅无法发挥作用，甚至可能因引入额外开销而导致压缩后体积增大。
因此，这些技术无法作为通用算法直接集成到处理混合数据类型的云存储去重工作流中。


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{../figures/0208segment.png}
%     \bicaption{索引合并技术示意图\cite{fu2015design}}{Illustration of index merging techniques}
%     \label{fig:cache_optimization}
% \end{figure}

\section{缓存优化技术}


哈希索引的管理是去重系统面临的另一大挑战。随着存储数据量的爆炸式增长，指纹索引的规模也在不断扩大，带来了存储和查询效率的压力~\cite{1014062863}。
为了解决这一问题，学术界和工业界提出了多种高效的索引优化与缓存技术。例如，使用布隆过滤器（Bloom Filter）~\cite{bloom1970space}及其
变种（如级联布隆过滤器）作为内存中的快速查询过滤器，可以有效拦截绝大多数新数据的查询请求，仅让少量可能重复的数据访问磁盘上的完整索引。
此外，基于数据局部性原理的容器化管理（Container-based Management）~\cite{zhu2008avoiding}和
索引分区（Index Partitioning）等策略，通过将相似或相关的数据块及其指纹组织在一起，优化了磁盘 I/O 模式，
进一步提升大规模索引的查询性能。



\section{本章小结}
本章首先深入剖析了传统去重系统的工作原理，详细探讨了数据分块、哈希指纹与索引、以及数据压缩等关键技术环节，并指出了其在面对新数据场景时暴露出的局限性。
接着，本章介绍了为克服传统去重技术限制而发展的细粒度去重技术，特别是增量压缩的原理、应用与挑战，同时也提及了 Burst 等面向特定场景的细粒度去重方法。
随后，本章概述了模型压缩技术的发展现状，区分了有损（剪枝、量化）与无损压缩方法，并重点分析了 ZipLLM、ZipNN 等模型专用无损压缩技术在云存储环境中的适用性与局限性。
此外，本章还简要讨论了去重系统中的缓存与索引优化技术。
总体而言，本章通过对相关技术的全面梳理，为后续章节中混合数据类型的细粒度冗余识别的数据缩减方法，奠定了坚实的理论基础，并明确了当前研究的技术需求。