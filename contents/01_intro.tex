% !TEX root = ../main.tex

\chapter{绪 论}


\section{研究背景与意义}
云计算技术的成熟与普及，已深刻地重塑了现代信息技术产业的格局。作为云计算服务的基石，云存储系统承担着
为全球海量用户提供可靠、可扩展数据存储能力的核心使命。随着数字化转型浪潮席卷各行各业，以及大数据、
人工智能等技术的广泛应用，全球数据总量正经历爆炸式增长。企业级应用、个人存储、物联网设备以及科学
计算等场景，每时每刻都在向云端注入天量数据，使得存储成本成为云服务厂商运营支出中最主要的构成部分之一。

为了有效应对大规模数据带来的成本压力，数据缩减技术——尤其是重复数据删除与压缩——自诞生以来便成为云存储系
统中不可或缺的核心模块。其中，基于固定长度分块的重复数据删除技术因其实现简单、性能稳定而得到最为广泛
的应用。其技术范式可概括为三个核心步骤：首先，将连续的数据流切割成固定大小的数据块（典型大小为4KB至
128KB）；继而，为每个数据块计算一个唯一的密码学哈希值（如SHA-1或SHA-256），作为该数据块的“指纹”；
最后，在系统内维护一个全局的指纹库，通过比对指纹来识别重复数据块：若指纹已存在，则仅存储一个指向
已有数据块的元数据指针；若指纹不存在，则对该新数据块进行压缩后存入物理介质，并将其指纹加入库中。这套
“分块—指纹—去重-压缩”的技术体系，在过去以文档、代码、虚拟机镜像等文本类数据为主的时代取得了辉煌的成
功，能够在这些场景下实现30\%至50\%乃至更高的存储空间节省，为云存储的规模化与商业化奠定了坚实的技术基础。

然而，近年来，随着人工智能，特别是深度学习技术的突破性进展，云存储所承载的工
作负载正在发生一场静默却深刻的革命。一个日益凸显的趋势是：存储池中的数据构成正从相对单一的文本类型
，转变为传统数据与AI模型数据共存的混合体。以.pt, .safetensors等格式保存的模型参数文件，其体积动
辄达到数百GB甚至TB级别，在总存储容量中的占比急剧攀升，已成为不可忽视的“新常态”数据。与此同时，为了
追求极高的I/O吞吐性能以满足高性能计算和大数据分析的需求，现代存储系统倾向于采用更大的数据块尺寸，
MB级别的数据块处理已在生产环境中屡见不鲜。

正是在这种新的数据特征下，沿用多年的传统数据缩减技术体系开始显现疲态，其效能出现断崖式下滑。在
不少混合负载场景中，整体的存储节省率已从过去的50\%左右暴跌至10\%以下。

首先，对于海量的AI模型数据，传统去重与压缩技术双双“失灵”。

去重失灵：模型文件由亿万级相互独立的浮点数张量构成，其数据内容具有高度的随机性和唯一性。
当系统将其切分为固定大小的块时，几乎每一个数据块都会拥有一个独一无二的指纹，这使得基于全局指
纹匹配的重复数据删除机制彻底失效，去重率趋近于零。

压缩失灵：诸如LZ4、Zlib、Zstandard等通用压缩算法，其核心原理是寻找并消除字节序列层面的
重复模式。然而，模型数据的冗余并不体现在连续的字节流中。其真正的冗余源于浮点数的数值分布特征与内
部结构。具体而言，在32位单精度浮点数中，其二进制表示由符号位、指数位和尾数位构成。在训练好
的模型中，绝大部分权重参数的数值都集中在某个较小的区间内（例如[-2, 2]），这导致其指数位的前几位
高度相似甚至完全相同。此外，由于量化或模型结构带来的规律性，在字节层面上可能呈现出以2字节或
4字节为周期的“循环相似”模式。(bit熵值)这种高位值重复、数值分布集中的“数值冗余”，是传统字节级压缩算法
无法理解和利用的。

其次，对于尺寸日益增大的传统数据块，传统去重技术的效果也大打折扣。

将数据块从KB级提升到MB级，虽然提升了I/O吞吐的连续性，但也极大地降低了找到两个完全一致的大数据
块的概率。然而，这并不意味着大块数据内部没有冗余。恰恰相反，在日志文件、备份集、版本库等场景中，
大块数据内部往往存在着大量局部性、连续性的重复片段（例如，同一时段产生的相同错误日志、文档中重复
出现的模板段落、代码库中相同的函数实现等）。传统以整个块为单位的“粗粒度”去重，造成了
冗余识别能力的巨大浪费。


综上所述，当前云存储数据缩减技术所面临的困境，其症结并非在于数据本身没有压缩空间，而是在
于传统技术冗余识别的“粒度”不足。无论是模型数据中隐含的“数值冗余”，还是大块传统数据中存在的
“局部连续冗余”，都超越了传统块级指纹比对技术的感知范围。因此，突破这一瓶颈的关键路径在于，
从“块级”的粗放式处理，迈向 “细粒度冗余识别” 的新范式。

然而，一个随之而来的严峻挑战是：模型数据与非模型大块数据所蕴含的细粒度冗余，其类型、结构与产
生机理截然不同。针对一种冗余类型的优化算法，很可能对另一种类型的数据产生“负优化”效应（负优化图）。例如，
专门为提取浮点数指数位冗余而设计的编码算法，如果被错误地应用于文本数据，由于其破坏了文本中原有的
字节序列重复模式，反而可能导致压缩率低于不使用任何优化；反之，专为发现文本数据块内部局部重复而设计的算
法，对模型数据则完全无效，反而增加了元数据存储和计算开销。这种在混合数据流中优化策略相互干扰，使得简单地堆砌多种
优化算法变得不可行。

至此，问题的全貌已然清晰：我们既需要更细粒度的冗余识别技术来挖掘新型负载中的压缩潜力，又必
须避免不同优化技术在混合场景下的相互干扰。面对这一两难问题，本研究提出了一条核心解决思路：“
鉴别数据，分而治之”。

具体而言，我们设想在传统的数据处理流水线中，增加一个智能的、轻量级的数据鉴别层。该层的任务是
实时地、精准地对每一个数据块进行分析，判断其更接近于“模型数据”还是“非模型传统数据”。在此基础上
，系统将实施 “分治”策略 ：对鉴别出的模型数据块，采用专门为之设计的浮点数冗余提取与编码技术，
消除其数值冗余；对鉴别出的传统数据块，则采用增强的、面向块内局部连续重复的检测与消除技术。通
过这种“先鉴别，后处理”的协同架构，我们有望在不过分影响系统吞吐性能的前提下，精准地对不同类型的
数据施加最有效的优化手段，从而系统性地提升混合负载下的整体去重率，攻克传统技术在AI时代面临的技术瓶颈。

\section{国内外研究现状}
\subsection{传统块级去重技术}
传统块级去重技术作为云存储系统中数据缩减的基石，其核心原理是通过消除重复数据块来降低存储开销。根据分块策略的不同，主要分为固定大小分块与可变长度分块两种技术路线。固定大小分块方案将数据流均分为固定长度的块（如4KB-128KB），并为每个块计算密码学哈希值（如SHA-256）作为唯一标识符。当新数据块与已有块的哈希值匹配时，系统仅存储指向原块的指针而非数据本身，从而实现存储空间的节省。这种方案实现简单，性能开销可控，已成为多数商业存储系统的标准配置，如Windows Server的Data Deduplication服务即采用此技术。

然而，固定分块方式存在明显的边界效应——数据在分块边界处的微小变化会导致后续所有分块都不匹配，严重降低去重效率。为克服这一局限，可变长度分块技术应运而生。此类技术利用Rabin指纹等滚动哈希算法，根据数据内容本身动态确定分块边界，使分块结果对数据插入和删除操作更具弹性。不过，可变长度分块的计算复杂度较高，可能需要更多的系统资源，在实践中有其适用场景的考量。

值得注意的是，这些传统块级去重技术在面对新兴的混合负载时，均暴露出了根本性局限。无论是固定还是可变分块，它们都依赖于寻找完全相同的字节序列，而对于AI模型文件中普遍存在的数值相似性冗余（如浮点数指数位高位相同但尾数位不同的情况），传统方法完全无法识别。同时，为提升I/O吞吐而采用的MB级大数据块，虽然减少了哈希计算和元数据管理开销，但也使得块内大量存在的局部连续重复无法被检测和消除。这些局限性促使研究者探索更细粒度的数据缩减技术路径。
\subsection{通用压缩算法研究进展}
在块级去重之后，通用压缩算法作为数据缩减流水线的第二道工序，负责进一步消除数据块内部的统计冗余。近年来，压缩算法的研究主要围绕速度与压缩率的平衡展开，产生了LZ4、Zstandard等代表性工作。

LZ4算法以其极致压缩速度著称，它通过优化LZ77的哈希链匹配策略与简化编码格式，实现了单线程500-800MB/s的压缩速率，解压速度甚至可达1GB/s以上。这种特性使其非常适合实时日志写入等高吞吐场景。然而，LZ4在压缩率方面做出了妥协，通常低于传统GZIP算法。

与LZ4追求极致速度不同，Zstandard算法致力于在速度与压缩率间取得平衡。它引入了有限状态熵编码(FSE)和预训练字典机制，在级别1时压缩速率可达300-400MB/s，级别10时仍能保持50-100MB/s，同时压缩率接近甚至超过GZIP。特别值得关注的是，Zstandard的解压速度与压缩级别无关的特性，使其非常适合长期归档的日志存储场景，天翼云在冷数据归档中采用Zstandard与去重联动，使归档数据存储密度提升了10倍。

尽管这些通用压缩算法对文本类数据表现出色，但它们对AI模型数据的压缩效果极为有限。根本原因在于，此类算法基于字节序列重复模式进行压缩，而模型数据中的浮点数张量虽然数值相近，但在字节表示上可能完全不同，导致传统压缩算法难以捕获其内在的数值冗余。这种局限性催生了面向新型数据特征的细粒度冗余识别技术。

\subsection{细粒度冗余识别技术}
增量压缩技术旨在解决传统去重无法处理的高度相似但不完全相同的数据块问题。该技术路线的核心思想是通过识别数据块间的相似性，仅存储差异部分（Delta），以实现压缩存储。其技术流程通常包括特征提取、相似匹配与差分计算三个关键步骤。

如何快速、准确地从海量数据块中找出相似块对，是该领域的核心挑战。N-Transform 是早期代表性工作，它基于Broder定理和Minhash思想，利用Rabin指纹并通过多次线性变换构建特征向量，以提高相似性检测的准确性。但其计算开销巨大，成为系统性能瓶颈。
Finesse 和 Odess 是针对N-Transform的改进方案。Finesse采用分组策略，将数据块划分为子块并提取特征，以提升效率。Odess则引入了内容定义采样方法，仅对一小部分哈希集进行耗时变换，在保证精度的同时，特征生成速度相比N-Transform提升了19.9倍。

在找到相似块后，如何高效生成差异补丁是关键。除了经典的Xdelta、BSDiff等算法，Xdelta和Gdelta代表了两种不同的技术优化路线。Xdelta采用经典的字符串匹配算法，通过动态规划寻找最优的编辑路径，生成的差异补丁最为紧凑，但计算开销较大。Gdelta则引入了启发式搜索策略，牺牲一定的压缩率来换取计算效率，特别适合对延迟敏感的应用场景。

除了面向相似块的增量压缩，另一研究方向专注于消除大块数据内部的局部连续重复，Burst是这方面的典型代表。与传统块级去重不同，Burst技术在MB级大块内部进行变长切分或滑动窗口匹配，识别并消除其中的长重复序列，如日志文件中的重复条目或文档中的相同段落。Burst的创新在于其能够突破固定块大小的限制，在更细粒度上发现冗余，这对于处理现代存储系统中的大块数据尤为有效。而Burst通过优化局部重复检测算法，在保证性能的同时提升了对传统数据的去重效率。

值得注意的是，这些细粒度冗余识别技术虽然提升了去重率，但也带来了额外的计算开销和系统复杂性。在混合负载环境中，若不对数据类型加以区分，盲目应用这些技术可能适得其反——例如，
将增量压缩用于本就压缩率低的模型数据，或将对局部重复有效的Burst技术用于全局随机的模型文件，都可能导致系统资源浪费而收效甚微。


\subsection{模型专用压缩技术}
面对AI模型数据的独特特性，研究者开发了多种模型专用压缩技术，其中量化和剪枝是最为重要的两种方向。模型量化通过降低权重和激活值的数值精度来减少存储占用和计算开销，从FP32降至INT8甚至INT4，可实现理论上的4倍至8倍压缩。

量化技术主要分为训练后量化(PTQ)和量化感知训练(QAT)。PTQ在模型训练完成后统计权重分布并映射到低精度表示，实现简单但可能带来精度损失；QAT在前向传播中使用低精度而反向传播保持FP32，梯度更新浮点权重，通常精度损失更小(<1\%)。GPT-QModel是这方面的先进工具，支持多种硬件平台的加速推理，集成了AWQ、Marlin等先进量化内核。

模型剪枝则通过移除对输出贡献微小的冗余参数来实现模型压缩。非结构化剪枝将单个不重要的权重置零，产生稀疏矩阵；结构化剪枝则按通道、卷积核或神经元粒度进行删除，保持稠密计算。研究表明，深度卷积神经网络存在大量参数冗余，通过剪枝可去除相当比例的参数而几乎不影响模型精度。

除了通用的量化和剪枝技术，研究者还开发了专为模型数据设计的压缩方法。
ZipNN是一种面向神经网络模型的专用压缩工具，它利用模型权重中存在的数值分布规律进行压缩。与传统压缩算法不同，ZipNN通过分析权重矩阵的数值特性，采用专门的编码策略，对模型文件能达到比通用算法更高的压缩率。
FM-Delta是一种面向海量微调基础模型的无损压缩方案。核心创新在于发现微调模型与预训练模型参数差异微小，通过参数整数化映射和熵编码技术，仅存储模型间差值。实验表明该方法减少了存储开销，实现无损压缩且耗时几乎不变。
ZipLLM针对大语言模型存储爆炸问题，协同整合了张量级去重与无损压缩：首先通过模型结构感知的张量去重消除冗余，降低元数据开销；引入“比特距离”指标自动聚类相似模型，进而提出BitX压缩算法，利用微调模型与基础模型间的比特级相似性，通过XOR运算生成稀疏差异再压缩，实现高比率无损压缩。
\subsection{研究现状总结}
当前数据去重与压缩技术研究从传统块级去重到细粒度冗余识别，再到模型专用压缩，技术路线不断细化深入。然而，在混合负载成为新常态的云存储环境中，现有技术仍面临诸多挑战。
首先，混合负载下的适配问题尚未得到很好解决。传统优化方案多为单一场景设计，在混合数据流中相互干扰——如专为模型数据设计的压缩方法对传统数据产生负优化，而面向文本冗余的消除技术对模型数据无效。这种相互掣肘的现象导致简单地堆砌多种优化算法变得不可行。

其次，现有细粒度冗余识别技术虽提升了去重率，但计算开销和系统复杂性仍然是阻碍其广泛应用的因素。N-Transform等方法的性能瓶颈、增量压缩对系统资源的较高要求，都使企业在实际部署时望而却步，特别是在性能敏感的生产环境中。

第三，模型压缩技术在追求高压缩率的同时，往往忽视通用性和系统集成。多数工具仅接收特定格式的模型文件输入，无法处理打包文件或识别模型文件中的非模型部分（如文件头、量化内容），限制了其在真实备份场景中的应用。

综上所述，当前数据缩减技术研究迫切需要一种能够智能识别数据类型并自适应应用最优策略的新型架构，从而在混合负载环境下实现整体去重率的最大化，同时维持可接受的性能开销。这正是本研究工作的出发点和创新方向。

\section{本文研究工作与主要贡献}
随着云计算与人工智能技术的深度融合，现代云存储系统正面临前所未有的数据缩减挑战。传统的基于块级指纹匹配的重复数据删除技术，在应对新兴的混合工作负载时效能急剧下降。本研究通过深入分析发现，这一问题的本质在于传统技术体系与新兴数据特征之间的"粒度失配"——传统粗粒度冗余识别方法无法有效捕捉AI模型数据中的数值相似性冗余，也难以利用大块传统数据内部的局部连续重复模式。

更为严峻的是，针对单一数据类型优化的先进技术（如面向模型数据的专用压缩和面向传统数据的细粒度去重）在混合负载环境下会产生相互干扰，导致整体性能下降。这一发现揭示了当前研究领域的一个重要空白：缺乏一种能够智能区分数据类型并自适应应用最优缩减策略的统一框架。

基于此，本研究的核心任务是设计并实现一套面向混合负载的细粒度冗余识别系统，该系统需解决三个关键问题：

（1）如何在数据流中实时、准确地鉴别不同冗余类型的数据块；

（2）如何针对不同类型的细粒度冗余设计高效的消除策略；

（3）如何在不显著影响系统吞吐的前提下，实现各种优化策略的协同工作。

基于对上述问题的分析，本文创新性地提出了名为FHRD (Fine-grained Hybrid Redundancy Deduplication)的细粒度冗余识别重复数据删除系统架构，其核心设计理念是"先鉴别，后分治"。如图3-1所示，该系统在传统数据缩减流水线的基础上，引入了智能鉴别层和差异化处理层，形成了模块化、可扩展的系统架构。数据块经过冗余模式鉴别探针，进行快速类型分析；基于鉴别结果，系统将数据块路由至相应的处理模块；最后，处理完成的数据与元数据一同存储。

本文的工作和贡献主要体现在以下四个方面：

1.本研究系统性地分析了传统数据缩减技术在AI时代混合负载环境下失效的原因，揭示了新时期两种影响明显的细粒度冗余模式——模型数据中的数值相似性冗余和大块数据中的局部连续冗余——的特征与分布规律。

2.提出了基于内容特征的块级数据类型鉴别方法，克服了传统依赖文件元数据进行类型判断的局限性。该方法的核心创新在于发现了模型数据在字节层面的可检测特征——以2/4字节为周期的循环相似模式，并设计了相应的轻量级检测算法。与现有方法相比，我们的鉴别机制具有精度高、适用性广的显著优势。特别值得一提的是，该方法能够准确识别出模型文件中的非模型部分（如文件头、量化内容），避免了对此类数据误用专用编码器而导致的负优化，这一能力在当前同类研究中尚未见报道。

3.面向模型数据的数值冗余编码器：专为消除AI模型文件中浮点数张量冗余而设计的压缩组件。其核心原理是基于浮点数格式的位结构分析，识别并分离出其中高度相似的指数位部分。编码器首先解析数据块中的浮点数序列，依据鉴别模块的结论，通过字节操作提取出具有共同模式的指数位区域，并将其重组为紧凑的冗余密集区，达到讲不可压缩数据块编码为可压缩数据块的效果，提升了模型数据的压缩率。且因其非侵入式设计，可直接嵌入现有压缩流水线，实现对混合存储场景中模型数据的精准优化。

4.面向传统大块数据的增强去重器：针对MB级大块内部局部连续冗余而设计的专用组件。其核心原理是突破传统固定块匹配的限制，通过滑动窗口机制与内容定义切分技术，在单个大块内部识别并消除细粒度的长重复序列。该组件对数据块进行局部指纹计算与匹配，定位其中完全相同的字节片段（如日志中的重复条目、文档中的相同段落），并以指针替代存储。有效弥补了大块化导致的去重率下降问题。

5.系统性实验验证：本研究基于开源去重系统Destor实现了完整的FHRD原型，并进行了系统性的实验验证。我们不仅证明了系统在去重率方面的显著提升（在混合负载下相比传统方法提高20\%-30\%），还详细分析了各模块的性能特征和系统整体开销。特别值得强调的是，我们的实验涵盖了多种真实场景，包括不同模型数据比例、不同块大小配置、不同文件封装形式等，提供了全面的混合负载数据缩减评估结果。
\section{论文结构}
%更加详细地介绍本文的组织结构。
本文内容主要分为六个章节，每章节安排如下：

第一章为绪论。首先说明研究背景与研究意义，分析云存储在人工智能时代面临的新挑战，指出传统块级去重与通用压缩在混合负载下的局限性；继而总结本文的研究目标与主要创新点，提出“先鉴别，后分治”的FHRD总体思路；最后概述论文的组织结构与各章节之间的逻辑关系，为读者阅读后续章节提供路线图。

第二章为相关工作。对国内外在数据去重、通用压缩、细粒度冗余识别以及模型专用压缩等方向的代表性工作进行系统回顾与对比分析。具体包括：固定分块和可变分块去重方法的原理与工程实现、主流压缩算法（如LZ4、Zstandard）的设计取舍、增量压缩与基于特征的相似性检测技术的优缺点，以及近年来针对深度学习模型的量化、剪枝与专用编码方法。通过归纳现有方法的适用场景与不足，为本文方法的设计动机提供理论与实证依据。



第三章为系统设计。详尽阐述FHRD系统的整体架构、模块划分与工作流。首先给出系统设计目标与性能约束（如可接受的延迟、吞吐与元数据开销）；然后介绍鉴别模块的设计思路与判别特征，说明如何在轻量化前提下实现高精度的数据类型判定；接着描述面向模型数据的数值冗余编码器，包括浮点格式分析、指数位/尾数位分离与重组策略；最后介绍面向传统大块数据的增强去重器，包含局部滑动窗口、内容定义切分与局部指纹匹配机制，并讨论模块间的调度与协同策略以避免负优化。

第四章为系统实现。给出FHRD原型的工程实现细节与关键算法的伪代码。包括鉴别算法的具体实现步骤与复杂度分析、数值冗余编码器的字节级操作实现与边界处理、增强去重器的索引与缓存设计，以及元数据格式和持久化策略。还将描述在现有去重系统（如Destor）中集成FHRD的具体改动、并行化与异步机制的实现以及对系统可配置参数的实现方式。

第五章为实验评估。详细介绍实验平台、数据集构成、基线方法与评价指标（如去重率、压缩率、吞吐、延迟与CPU/内存开销）。通过一系列对比试验评估FHRD在不同混合负载比例、不同块大小与不同模型文件封装形式下的性能表现，并分析各模块对总体效果的贡献度与敏感性。实验部分还将针对负优化情形给出消融研究，验证鉴别模块与分治策略在避免互相干扰方面的有效性。

第六章为总结与展望。首先总结本文的主要研究成果与工程实践经验，回顾FHRD在混合负载下提升去重效率与控制性能开销的关键因素；随后讨论当前方法的局限性与潜在改进方向，例如鉴别精度在更广泛模型格式下的鲁棒性、编码器与去重器的硬件加速途径、以及系统在分布式大规模部署中的一致性与可扩展性问题；最后展望未来在自适应策略、多模态数据处理以及与存储云平台深度集成方面的研究方向。


% Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
% incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
% nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
% Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
% fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
% culpa qui officia deserunt mollit anim id est laborum.

% \section{脚注}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
% incididunt ut labore et dolore magna aliqua. \footnote{Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum
% dolore eu fugiat nulla pariatur.}

% \section{字体}

% 上海交通大学是我国历史最悠久的高等学府之一，是教育部直属、教育部与上海市共建的全
% 国重点大学，是国家“七五”、“八五”重点建设和“211 工程”、“985 工程”的首批建
% 设高校。经过 115 年的不懈努力，上海交通大学已经成为一所“综合性、研究型、国际化”
% 的国内一流、国际知名大学，并正在向世界一流大学稳步迈进。 

