% !TEX root = ../main.tex

\chapter{绪 论}


\section{研究背景与意义}
云计算技术的成熟与普及，深刻地重塑了现代信息技术产业的格局。作为云计算服务的基石，云存储系统承担着为全球海量用户提供可靠、
可扩展数据存储能力的核心使命。随着数字化转型浪潮席卷各行各业，以及大数据、人工智能等技术的广泛应用，全球数据总量正经历爆炸式增长。
企业级应用、个人存储、物联网设备及科学计算等场景，每时每刻都在向云端注入海量数据，
使得存储成本成为云服务提供商运营支出中最主要的部分之一~\cite{khan2024cloud}。

为了有效应对大规模数据带来的成本压力，数据缩减技术——尤其是重复数据删除（Deduplication）与压缩（Compression）
——自诞生以来便成为云存储系统中不可或缺的核心模块~\cite{dubois2011key}。其中，基于内容定义分块的重复数据删除技术
因其能有效处理数据偏移问题而得到广泛应用。其技术范式可概括为：首先，将连续的数据流切割成可变长度的数据块；其次，
为每个数据块计算唯一的密码学哈希值（如SHA-256）作为其“指纹”；最后，在系统内维护一个全局指纹库，通过比对指纹识别重复数据块。
若指纹已存在，则仅存储一个指向已有数据块的元数据指针；若指纹不存在，则对该新数据块进行压缩后存入物理介质，
并将其指纹加入库中~\cite{xia2016comprehensive}。这套“分块—指纹—去重—压缩”的技术体系，
在过去以文档、代码、虚拟机镜像等传统数据为主的时代取得了巨大成功，在这些场景下能实现30\%至62\%乃至更高的存储空间节省，
为云存储的规模化与商业化奠定了坚实的技术基础。

然而，近年来随着人工智能，特别是深度学习技术的突破性进展，云存储所承载的工作负载正发生一场深刻的变革。
一个日益凸显的趋势是：存储池中的数据构成正从相对单一的传统数据类型，演变为传统数据与AI模型数据共存的混合负载。
以 \texttt{.pt}、\texttt{.safetensors} 等格式保存的模型参数文件，其体积动辄达到数百GB甚至TB级别，在总存储容量中的占比急剧攀升，
已成为一种“新常态”数据~\cite{wang2026zipllm}。与此同时，为了追求极高的I/O吞吐以满足高性能计算和大数据分析的需求，
现代存储系统倾向于采用更大的数据块尺寸，MB级别的数据块在分布式生产环境中已屡见不鲜~\cite{borthakur2008hdfs}。

正是在这种新的数据特征组合下，沿用多年的传统数据缩减技术体系开始显现其局限性。首先，对于海量的AI模型数据，传统去重与压缩技术几乎同时失效：

\begin{itemize}
    \item \textbf{去重失效}：模型文件由亿万级相互独立的浮点数张量构成，数据内容呈现高度的随机性和唯一性。
    当系统将其切分为数据块时，几乎每个块都会拥有一个独一无二的指纹，这使得基于全局指纹匹配的去重机制完全失效，去重率趋近于零。
    \item \textbf{压缩失效}：诸如LZ4、Zstandard等通用压缩算法，其核心原理是寻找并消除字节序列层面的重复模式。
    然而，模型数据的冗余并非体现在连续的字节流中，而是源于浮点数的数值分布特征与内部结构。具体而言，在32位单精度浮点数中，
    其二进制表示由符号位、指数位和尾数位构成。在训练好的模型中，绝大部分权重参数的数值集中在某个较小区间内（例如[-2, 2]），
    导致其指数位的前几位高度相似甚至完全相同。此外，由于量化或模型结构带来的规律性，字节层面可能呈现以2字节或4字节为周期的“循环相似”模式。
    这种由高位重复、数值分布集中所构成的“数值冗余”，是传统字节级压缩算法难以捕捉和利用的~\cite{hershcovitch2025zipnn}。
\end{itemize}

其次，对于尺寸日益增大的传统数据块，传统去重技术的效果也显著下降~\cite{pengburst}。将数据块从KB级提升至MB级，
虽提升了I/O的连续性，但也极大地降低了找到两个完全相同大数据块的概率。然而，这并不意味着大块内部没有冗余。相反，
在日志文件、备份集、版本库等场景中，大块往往包含大量局部重复片段（如同一时段产生的相同错误日志、文档中重复的模板段落、
代码库中重复的函数实现等）。传统以整个块为单位的粗粒度去重，无法识别这些块内冗余，导致去重能力下降。

综上所述，当前云存储数据缩减技术面临的困境，其症结在于传统技术的冗余识别“粒度”与新兴数据特征不匹配。无论是模型数据中隐含的“数值冗余”，
还是大尺寸传统数据中存在的“局部连续冗余”，都超越了传统块级指纹比对技术的感知范围。因此，突破这一瓶颈的关键路径在于，从“块级”的粗放式处理，
迈向“细粒度冗余识别”的新范式。

然而，一个严峻的挑战随之而来：模型数据与非模型大块数据所蕴含的细粒度冗余，其类型、结构与产生机理截然不同。针对一种冗余类型设计的优化算法，
很可能对另一种类型的数据产生“负优化”效应。例如，专门为提取浮点数指数位冗余而设计的编码算法，若被错误地应用于文本数据，
会因破坏文本中原有的字节序列重复模式，反而导致压缩率降低；反之，专为发现文本数据块内部局部重复而设计的算法，对模型数据则完全无效，
同时增加了不必要的元数据存储和计算开销。这种在混合数据流中优化策略的相互干扰，使得简单堆砌多种优化算法的方案变得不可行。

至此，问题的全貌已然清晰：我们既需要更细粒度的冗余识别技术来挖掘新型负载中的压缩潜力，
又必须避免不同优化技术在混合场景下的相互掣肘。面对这一两难困境，本研究提出了一条核心解决思路：“鉴别数据，分而治之”。

具体而言，我们在传统的数据处理流水线中，增加一个智能、轻量级的数据鉴别层。该层的任务是实时、精准地对每个数据块进行分析
，判断其更接近于“模型数据”还是“非模型传统数据”，对鉴别出的模型数据块，采用专门为之设计的浮点数冗余提取与编码技术，
消除其数值冗余；对鉴别出的传统数据块，则采用增强的、面向块内局部连续重复的检测与消除技术。通过这种“先鉴别，后处理”的协同架构，我们有望在不过分影响系统吞吐性能的前提下，精准地对不同类型的数据施加最有效的优化手段，从而系统性地提升混合负载下的整体去重率，攻克传统技术在AI时代面临的技术瓶颈。

\section{国内外研究现状}
数据去重与压缩技术的研究已历经数十年的发展，从传统的块级去重到细粒度的冗余识别，再到针对特定数据类型的专用压缩，技术路径呈现出不断细化与深入的趋势。

传统块级去重技术是云存储系统中数据缩减的基石，其核心原理是通过消除重复数据块来降低存储开销。
根据分块策略的不同，主要分为固定大小分块与可变长度分块两种技术路线。固定大小分块方案将数据流均分为固定长度的块，实现简单，
性能开销可控，但存在明显的边界效应——数据在分块边界处的微小变化会导致后续所有分块都不匹配，严重降低去重效率~\cite{quinlan2002venti}。
为克服这一局限，可变长度分块技术应运而生，它利用Rabin指纹等滚动哈希算法，根据数据内容本身动态确定分块边界，
使分块结果对数据插入和删除操作更具弹性~\cite{muthitacharoen2001low}。然而，无论是固定还是可变分块，
它们都依赖于寻找完全相同的字节序列，对于AI模型文件中普遍存在的数值相似性冗余，传统方法完全无法识别。
同时，为提升I/O吞吐而采用的MB级大数据块，也使得块内大量存在的局部连续重复无法被检测和消除。

在块级去重之后，通用压缩算法，如Gzip \cite{gailly1992gnu}、LZW \cite{nelson1989lzw}、ANS \cite{duda2015use}，作为数据缩减流水线的第二道工序，负责进一步消除数据块内部的统计冗余。近年来，压缩算法的研究主要围绕速度与压缩率的平衡展开，
产生了如LZ4\footnote{\url{https://github.com/lz4/lz4}} 和Zstandard~\cite{collet2018zstandard}等新一代压缩算法。LZ4以其极致的压缩和解压速度著称，适合实时
高吞吐场景，但压缩率相对较低。Zstandard则致力于在速度与压缩率间取得更优平衡，通过引入有限状态熵编码（FSE）和预训练字典等机制，
提供了灵活的压缩级别选择，在保持高速解压的同时实现了接近甚至超越传统Gzip的压缩率。尽管这些通用压缩算法对文本类数据表现出色，
但它们对AI模型数据的压缩效果极为有限，其根本原因在于它们基于字节序列重复模式进行压缩，无法捕获模型数据内在的数值冗余。

% \subsection{细粒度冗余识别技术}

为突破传统块级去重在大块场景下的局限，学术界提出了多种细粒度冗余识别技术，主要可分为两大
类：面向相似块的增量压缩技术和面向局部连续重复的细粒度去重技术。增量压缩技术旨在通过识别数据块间的相似性，
仅存储差异部分（Delta）来实现压缩。其核心挑战在于如何快速、准确地从海量数据块中找出相似块对。从早期的N-Transform~\cite{broder1997resemblance}到
Finesse~\cite{zhang2019finesse}和Odess~\cite{xia2023design}，研究者们不断优化特征提取与匹配算法
以平衡精度与效率。找到相似块后，Xdelta~\cite{macdonald2000file}和Gdelta~\cite{tan2024design}等差异编码技术则用于
生成紧凑的补丁文件。另一研究方向专注于消除大块数据内部的局部连续重复，Burst~\cite{pengburst}是这方面的典型代表。它通过在MB级大
块内部进行识别并消除首尾长重复序列，有效缓解了因块尺寸增大导致的去重率下降问题。然而，这些细粒度技术在带来更高去重率的同时，
也引入了额外的计算开销和系统复杂性，在混合负载环境中若不加区分地应用，可能导致资源浪费。

面对AI模型数据的独特性，研究者提出了多种专用压缩技术，其中量化和剪枝是最为重要的两种方向。
模型量化~\cite{gholami2022survey}通过降低权重和激活值的数值精度来减少存储与计算开销，而模型
剪枝~\cite{ma2021effective}则通过移除冗余参数来实现模型压缩。这两类方法属于有损压缩，通常不适用于要求
数据完整性的云存储服务。此外，还涌现了如ZipNN~\cite{hershcovitch2025zipnn}、FM-Delta~\cite{ning2024fm}和ZipLLM~\cite{wang2026zipllm}等无损压缩方法，它
们利用模型权重间的数值分布规律或跨模型版本的比特级相似性，实现了比通用算法更高的压缩率。但这些方法往往只针对特定格式的模型文件，
无法处理打包文件或识别模型文件中的非模型部分（如文件头、量化内容），限制了其在真实备份场景中的应用。

综上所述，当前数据缩减技术研究面临三大挑战。
首先，混合负载下的适配问题尚未得到很好解决。专为单一场景设计的优化方案在混合数据流中可能相互干扰，产生负优化~\cite{wang2026zipllm}。
其次，现有细粒度冗余识别技术虽提升了去重率，但其计算开销和系统复杂性仍是广泛应用的障碍。
第三，模型专用压缩技术在追求高压缩率的同时，常忽视通用性和系统集成问题，难以适应真实的云存储环境。

因此，当前迫切需要一种能够智能鉴别数据类型、细粒度识别不同冗余模式、并在性能与效果间取得平衡的新型架构。这正是本研究工作的出发点和创新方向。

\section{本文研究工作与主要贡献}
随着云计算与人工智能技术的深度融合，现代云存储系统正面临前所未有的数据缩减挑战。传统的基于块级指纹匹配的重复数据删除技术，在应对新兴的混合工作负载时效能急剧下降。本研究深入分析发现，问题的本质在于传统技术体系与新兴数据特征之间的“粒度失配”——传统粗粒度冗余识别方法无法有效捕捉AI模型数据中的数值相似性冗余，也难以利用大尺寸传统数据内部的局部连续重复模式。

更为严峻的是，针对单一数据类型优化的先进技术（如面向模型数据的专用压缩和面向传统数据的细粒度去重）在混合负载环境下会产生相互干扰，导致整体性能下降。这一发现揭示了当前研究领域的一个重要空白：缺乏一个能够智能区分数据类型并自适应应用最优缩减策略的统一框架。

基于此，本研究的核心任务是设计并实现一套面向混合负载的细粒度冗余识别系统，旨在解决三个关键问题：
（1）如何在数据流中实时、准确地鉴别不同冗余类型的数据块；
（2）如何针对不同类型的细粒度冗余设计高效的消除策略；
（3）如何在不显著影响系统吞吐的前提下，实现各种优化策略的协同工作。

为解决上述问题，本文创新性地提出了名为FHRD (Fine-grained Hybrid Redundancy Deduplication) 的细粒度冗余识别去重系统架构。如图3-1所示，该系统在传统数据缩减流水线的基础上，引入了数据类型鉴别层和差异化处理层，形成了模块化、可扩展的系统架构。数据块经过冗余模式鉴别探针进行快速类型分析；基于鉴别结果，系统将数据块路由至相应的处理模块；最后，处理完成的数据与元数据一同存储。

本文的工作和贡献主要体现在以下几个方面：

\begin{enumerate}
    \item 揭示了AI时代混合负载下的新型细粒度冗余模式。本研究系统性地分析了传统数据缩减技术在现代云存储环境中失效的根源，明确了两种影响显著的新型冗余：模型数据中的“数值相似性冗余”与大块传统数据中的“局部连续冗余”，并阐述了它们的特征与分布规律。

    \item 提出了基于内容特征的高精度块级数据类型鉴别方法。该方法克服了传统依赖文件元数据进行类型判断的局限性，其核心创新在于发现了模型数据在字节层面呈现的“循环相似模式”，并设计了相应的轻量级检测算法。与现有方法相比，该鉴别机制具有精度高、适用性广的优势，且能够准确识别出模型文件中的非模型部分（如文件头、量化内容），避免了误用专用编码器导致的负优化。

    \item 设计并整合了面向混合场景的差异化冗余处理逻辑。针对模型数据的数值冗余，基于浮点数位结构分析，分离并重组高度相似的指数位部分，将不可压缩数据块转化为可压缩形式；针对大块数据的局部连续冗余，通过局部指纹计算与匹配，定位并消除细粒度长重复序列。通过这种方式，实现了对两类冗余的精准优化与整体去重效能的提升。

    \item 实现了完整的原型系统并进行了系统性的实验验证。本研究基于开源去重系统Destor \cite{fu2015design}实现了FHRD原型。实验结果表明，在混合负载下，FHRD相比传统方法在去重率上取得了20\%至30\%的显著提升。我们通过全面的实验，涵盖了不同模型数据比例、块大小配置及文件形式，详细分析了各模块的性能特征和系统整体开销，为混合负载下的数据缩减提供了有力的评估数据。
\end{enumerate}

\section{论文结构}
本文内容共分为六章，其组织结构如下：

第一章为绪论。阐述研究背景与意义，分析云存储在人工智能时代面临的新挑战，指出传统数据缩减技术在混合负载下的局限性。在此基础上，明确本文的研究目标与主要创新点，并概述论文的组织结构。

第二章为相关工作。系统回顾并对比分析数据去重、通用压缩、细粒度冗余识别及模型专用压缩等方向的代表性工作。通过归纳现有方法的适用场景与不足，为本文方法的设计动机提供理论与实证依据。

第三章为系统设计。针对现有技术的痛点，提出面向混合负载的FHRD系统总体架构。首先分析细粒度冗余特征，明确系统设计目标；继而详细阐述“块级数据类型鉴别”模块、模型数据编码方法及非模型数据细粒度子块匹配方法的设计；最后梳理系统工作流，为后续实现奠定基础。

第四章为系统实现与优化。聚焦系统落地过程中的核心挑战，给出具体的实现与优化方案。详细介绍字节粒度分组抽样熵值分析、基于熵值结论的字节分组压缩、指数取整的双向子块定长切分等核心方法的实现细节；同时，为提升系统整体性能，设计了数据段聚合、多级索引缓存及流水线并行处理等优化策略。

第五章为实验评估。搭建标准化实验平台，选取多样的非模型、模型及混合负载数据集，设置多种对照方案，从去重率、吞吐量、鉴别准确率等核心指标展开全面评估。通过基准实验、消融实验和参数敏感性实验，系统地验证FHRD在混合负载下的整体优势、各关键模块的贡献及其在不同场景下的适应性。

第六章为总结与展望。系统总结全文研究成果，提炼FHRD系统的核心技术创新与工程实践价值。同时，分析当前研究的局限性，并结合存储技术发展趋势，对未来的研究方向进行展望。

% Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
% incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
% nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
% Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
% fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
% culpa qui officia deserunt mollit anim id est laborum.

% \section{脚注}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
% incididunt ut labore et dolore magna aliqua. \footnote{Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum
% dolore eu fugiat nulla pariatur.}

% \section{字体}

% 上海交通大学是我国历史最悠久的高等学府之一，是教育部直属、教育部与上海市共建的全
% 国重点大学，是国家“七五”、“八五”重点建设和“211 工程”、“985 工程”的首批建
% 设高校。经过 115 年的不懈努力，上海交通大学已经成为一所“综合性、研究型、国际化”
% 的国内一流、国际知名大学，并正在向世界一流大学稳步迈进。 

