% !TEX root = ../main.tex

\chapter{相关技术概述}

\section{云存储服务}
云存储服务作为云计算基础设施的核心组成部分，已成为现代企业数据管理的关键支撑。它通过虚拟化技术将分布在大量异构存储设备上的物理资源进行整合与抽象，形成一个统一、可弹性扩展的逻辑存储资源池，从而为用户提供按需分配、动态伸缩、高可用且持久化的数据存储能力。与传统的本地存储架构相比，云存储通过网络提供服务，实现了计算与存储资源的解耦及共享，显著提升了资源利用率和系统部署的灵活性。

根据数据组织方式与访问接口的差异，云存储服务主要可分为以下三类：

\begin{enumerate}
    \item \textbf{对象存储}：对象存储采用扁平化的命名空间来组织数据，每个数据单元（对象）均通过一个全局唯一的标识符（Key）进行寻址，彻底摆脱了传统文件系统中复杂的目录层级限制。其典型代表包括 Amazon S3、阿里云 OSS 及 Google Cloud Storage 等。对象存储天然适用于海量非结构化数据（如图片、视频、日志、备份文件等）的存储场景，具备近乎无限的扩展能力、高持久性与优异的成本效益。为保证数据可靠性，对象存储系统通常在底层采用多副本或纠删码（Erasure Coding）等冗余策略，并提供标准化的 RESTful API，极大地简化了跨地域、跨平台的应用集成。
    \item \textbf{文件存储}：文件存储为用户提供了熟悉的、符合 POSIX 标准的树状目录结构与文件操作接口，特别适用于需要文件共享、协同编辑以及复杂目录结构的应用场景。Google File System (GFS) 及其后继者 Colossus 是该领域的开创性系统，而主流云服务商则提供如 Amazon EFS、Azure Files 等托管服务。这类系统通过将大文件切分为数据块并分散存储于多个节点，实现了高吞吐量与高容错性，因此广泛应用于大数据分析、高性能计算及机器学习训练等数据密集型应用。
    \item \textbf{块存储}：块存储将存储资源抽象为原始的、未格式化的裸设备块（Block），通过 iSCSI 等协议以网络附加存储（NAS）或存储区域网络（SAN）的形式挂载至计算节点。用户可以像使用本地硬盘一样，在这些块设备上创建文件系统、部署数据库或作为虚拟机的系统盘/数据盘。Amazon EBS、Azure Disk Storage 及开源的 Ceph RBD 均是典型的块存储实现。其核心优势在于提供低延迟、高 IOPS（每秒读写次数）的稳定性能，是数据库、实时交易系统等对 I/O 性能要求严苛的应用的理想选择。
\end{enumerate}

从技术架构的视角审视，典型的云存储系统（如图~\ref{fig:cloud_storage_architecture} 所示）普遍采用分层设计，自上而下依次为应用接口层、存储服务层与分布式存储基础层。

应用接口层作为系统的门户，通过标准化的 RESTful API 或特定协议的客户端（SDK）向最终用户及应用程序提供统一的服务入口，屏蔽了底层存储的复杂性。

存储服务层是系统的核心，负责将底层的物理存储资源抽象并封装为前述的对象、文件、块等不同类型的服务。至关重要的是，正是在这一层，云服务提供商集成了包括重复数据删除、数据压缩在内的关键数据缩减技术。在数据被持久化至物理介质前，这些技术能够智能识别并消除数据中的冗余信息，从而显著降低实际存储成本并提升数据在网络中的传输效率。

分布式存储基础层是整个架构的基石，通常由 HDFS~\cite{borthakur2008hdfs}、Ceph~\cite{weil2006ceph} 等成熟的分布式存储系统构成。该层负责数据的物理存储、通过多副本或纠删码策略实现数据冗余与高可用、处理节点故障与数据恢复，以及执行跨节点的负载均衡与资源调度等基础管理职能。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/01.pdf}
    \bicaption{云存储服务系统架构}{System architecture for cloud storage services}
    \label{fig:cloud_storage_architecture}
\end{figure}

这种清晰的分层架构，使得云存储系统能够灵活地同时满足高性能计算、大数据分析、备份归档等多种应用场景的需求。

在成本构成方面，存储硬件采购和维护成本占据了云服务商运营成本的较大比重。为了在激烈的市场竞争中保持价格优势，云服务商必须采用高效的数据缩减技术。数据缩减效果直接影响到云存储服务的利润率，这也是各大云服务商持续投入重金研发数据缩减技术的根本动力。Amazon S3 智能分层、阿里云高效云盘、腾讯云极速型云硬盘等产品都集成了先进的数据缩减技术，在保证性能的同时显著降低了存储成本。
\section{传统重复数据删除系统}
传统重复数据删除（Deduplication）系统，其架构如图~\ref{fig:dedup_architecture}所示，通常采用“先去重，后压缩”的两阶段处理流水线，核心目标在于通过消除数据冗余以最大化存储空间节省。这一经典架构由数据切块、指纹计算、匹配去重和数据压缩四个关键步骤构成，共同组成一个完整的数据缩减工作流。

系统处理流程始于\textbf{数据切块}（Chunking）阶段。在此阶段，原始数据流被分割成一系列较小且易于管理的单元，即数据块（Chunk）。

随后进入\textbf{指纹计算}（Fingerprinting）阶段。系统利用密码学哈希函数（如 \texttt{SHA-1} 或 \texttt{SHA-256}）为每个数据块生成一个唯一的数字指纹（Fingerprint）。这些指纹如同数据的“身份证”，是后续去重决策的唯一依据。

\textbf{匹配去重}（Duplicate Check）是系统的核心环节。系统维护一个全局的指纹索引库（Index），用于存储所有已处理过的数据块的指纹。当新数据块的指纹生成后，系统会查询索引库。若发现匹配的指纹，则表明该数据块是重复的，系统仅需存储一个指向已有物理数据块的元数据指针即可，无需再次存储相同的数据。反之，若未发现匹配，则该数据块被判定为唯一块（Unique Chunk），并被送入下一处理阶段。

最后，在\textbf{数据压缩}（Compression）阶段，所有唯一数据块会经过通用压缩算法（如 Gzip、LZ4、Zstd 等）的进一步处理。这一步骤旨在消除单个数据块内部的统计冗余，从而实现存储空间的二次优化。

这种“切块-指纹-匹配去重-压缩”的经典设计~\cite{wallace2012characteristics,meyer2012study}，使得传统重复数据删除系统能够在保证数据完整性的前提下，显著提升存储效率。然而，随着数据形态的不断演变，这一经典架构在面对新型工作负载时逐渐暴露出其局限性，尤其是在处理 AI 模型数据中普遍存在的数值冗余，以及大块数据内部的局部连续重复时，其处理能力显得捉襟见肘。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/传统重复数据删除系统.drawio.pdf}
    \bicaption{传统重复数据删除系统架构}{Architecture of traditional deduplication system}
    \label{fig:dedup_architecture}
\end{figure}

\subsection{数据切块技术}

% \paragraph{固定大小分块}
固定大小分块（Fixed-Size Chunking, FSC）是最基础、最直观的数据分块方法。该方法将输入的数据流视为一个连续的字节序列，并按照预设的固定长度（如 4KB、8KB 或 64KB）进行均匀切分，从而生成一系列大小完全相同的数据块~\cite{quinlan2002venti}。

FSC 的实现极为简单，计算开销极低。由于分块边界仅由数据在流中的偏移量唯一确定，无需进行任何复杂的内容分析，因此其处理速度具有显著优势。在数据内容相对静态的场景中，如虚拟机镜像的首次全量备份、大规模软件分发包的归档等，该技术能够提供稳定且可靠的去重效果。此外，大小均一的数据块也简化了存储空间的管理与分配，降低了系统设计的整体复杂性。

然而，FSC 技术存在一个致命的缺陷，即\textbf{边界偏移问题}（Boundary Shift Problem）。当数据流中发生任何插入或删除操作时，哪怕只是单个字节的变动，都会导致该变动点之后的所有数据块边界发生连锁式偏移。例如，在一个文本文件中部插入一行文字，将导致从插入点到文件末尾的所有后续数据块的内容全部改变。这种现象使得原本相同的大段内容，在新生成的数据流中因其所处位置的变化而被识别为全新的、不同的数据块。这种连锁反应严重削弱了去重系统识别重复内容的能力，从而导致去重效率急剧下降。

% \paragraph{内容定义切块}
内容定义切块（Content-Defined Chunking, CDC），又称变长切块，是为解决固定分块的边界偏移问题而设计的关键技术。其核心思想是根据数据内容自身的特征来动态确定分块边界，从而使得分块结果对数据的插入和删除操作具有良好的鲁棒性~\cite{muthitacharoen2001low}。

Rabin 算法~\cite{rabin1981fingerprinting} 是 CDC 技术中最具代表性且应用最广泛的一种实现。该算法采用一个固定大小的滑动窗口（Sliding Window）在数据流上逐字节滑动，并为窗口内的数据高效地计算一个哈希值，即 Rabin 指纹。当该哈希值的某些特定位（通常是低位）与一个预定义的模式（Pattern）匹配时，就将当前窗口的结束位置标记为一个切分点。Rabin 算法的核心优势在于其“滚动哈希”（Rolling Hash）的特性：当窗口向前滑动一个字节时，新的哈希值可以通过一次简单的数学运算从旧的哈希值推导得出，无需重新对整个窗口的数据进行完整计算，这使得 CDC 过程极为高效。

在实际应用中，CDC 算法需要精细地控制生成块的大小分布。理想情况下，块大小应在一个合理的范围内波动，既要避免产生大量过小的块（这会导致元数据急剧膨胀，增加管理开销），也要防止出现少数过大的块（这会降低发现重复数据的概率）。为实现这一目标，通常会设定一个最小块大小（Min-Size）和最大块大小（Max-Size）。只有当滑动窗口超过最小块大小时，才开始检查哈希值；而当窗口滑动距离超过最大块大小时，则无论哈希值是否匹配，都会强制进行一次切分。

\subsection{哈希指纹与索引}
哈希指纹是识别重复数据块的基石。一个理想的哈希函数需要满足以下条件：首先，对于相同内容的数据块，必须生成完全相同的指纹（确定性）；其次，对于不同内容的数据块，生成相同指纹的概率（即哈希碰撞）必须极低（抗碰撞性）；最后，哈希计算本身的速度要足够快，以满足高吞吐的数据处理需求。

在早期的去重系统中，\texttt{MD5} 和 \texttt{SHA-1} 等加密哈希函数因其良好的抗碰撞性而被广泛采用~\cite{srinivasan2012idedup,xia2011silo}。然而，随着计算能力的提升和安全标准的演进，这些算法的安全性受到挑战。因此，现代云存储系统逐渐转向使用如 \texttt{SHA-256} 等更为安全的哈希算法~\cite{drago2012inside}，尽管这会带来一定的计算成本增加~\cite{rachmawati2018comparative}。

哈希索引的管理是去重系统面临的另一大挑战。随着存储数据量的爆炸式增长，指纹索引的规模可能达到 TB 级别，远超单台服务器的内存容量。为了解决这一问题，学术界和工业界提出了多种高效的索引优化技术。例如，使用布隆过滤器（Bloom Filter）~\cite{bloom1970space}或其变种（如级联布隆过滤器）作为内存中的快速查询过滤器，可以有效拦截绝大多数新数据的查询请求，仅让少量可能重复的数据访问磁盘上的完整索引。此外，基于数据局部性原理的容器化管理（Container-based Management）~\cite{zhu2008avoiding}和索引分区（Index Partitioning）等策略，通过将相似或相关的数据块及其指纹组织在一起，优化了磁盘 I/O 模式，进一步提升了大规模索引的查询性能。

\subsection{压缩算法}
在重复数据删除流程之后，压缩算法作为数据缩减流水线的最后一道防线，负责进一步消除每个唯一数据块内部的统计冗余。

LZ 系列算法是基于字典编码原理的经典压缩方法，其通过查找并替换数据中重复出现的字符串来实现压缩。其中，LZ4 算法以其极致的速度而著称，它采用简化的哈希链机制和字节对齐的编码格式，实现了极高的处理吞吐量。在典型的 x86 平台上，LZ4 的压缩速度可达数百 MB/s，而解压速度更是接近 GB/s，使其成为实时数据处理、高速缓存等对延迟高度敏感场景的理想选择。

Zstandard（Zstd）算法则代表了速度与压缩率的极致平衡。它在经典的 LZ77~\cite{ziv2003universal} 算法基础上，创新性地引入了非对称数字系统（Asymmetric Numeral Systems, ANS）的一种高效实现——有限状态熵编码（Finite State Entropy, FSE）。FSE 能够以接近信息论香农极限的效率进行熵编码，同时保持对现代 CPU 硬件友好的特性。Zstandard 提供了从负到正共 22 个压缩级别，允许用户根据具体应用需求，在压缩速度与压缩率之间做出灵活的权衡。在默认级别下，Zstd 通常能提供比传统 Zlib 更高的压缩率，同时保持更快的压缩和解压速度。

在选择压缩算法时，系统需要综合考量数据类型、性能要求及硬件特性。例如，文本、日志等非结构化数据通常具有较高的统计冗余，能够获得良好的压缩效果；而已经过压缩的媒体文件（如 JPEG 图片、MP4 视频）或加密数据，其内容近似于随机噪声，压缩空间极为有限。在某些对 I/O 延迟极度敏感的应用场景中，系统甚至会选择完全禁用压缩，以避免引入额外的 CPU 计算开销。

\section{细粒度去重技术}
传统重复数据删除技术基于精确匹配机制，仅能识别和消除完全一致的数据块，对于存在高度相似性但并不完全相同的数据块（例如，文档的修订历史、虚拟机镜像的连续版本）处理效果有限。为解决这一问题，增量压缩技术应运而生，它通过识别并利用数据块之间的相似性，仅存储它们之间的差异，从而突破了精确匹配的限制。

典型的增量压缩系统包含三个关键环节：特征提取、相似匹配和差分计算：

\begin{itemize}
    \item \textbf{特征提取}：为每个数据块计算一个或多个能够代表其内容的“特征”。理想的特征应保证语义相似的块具有相似的特征。
    \item \textbf{相似匹配}：通过快速比对新数据块的特征与索引中已有特征的相似度，寻找最相似的候选参考块。
    \item \textbf{差分计算}：在找到参考块后，利用差分算法（如 \texttt{xdelta}）计算目标块与参考块之间的精确差异，生成紧凑的补丁文件。
\end{itemize}

增量压缩技术在备份服务~\cite{xia2014combining}、分布式存储~\cite{shilane2012wan}和容器镜像分发等场景中展现出显著价值。然而，其计算复杂度高、系统设计复杂，且在处理 AI 模型等缺乏明显相似性的数据时，可能因不必要的计算开销而产生负面效果。

除增量压缩外，研究者还提出了多种其他细粒度去重技术。例如，Burst~\cite{pengburst}专注于解决大块数据内部的局部重复问题，通过在块内进行冗余消除，特别适合处理日志文件等包含大量局部重复的数据。此外，基于语义的去重技术，如 Apache ORC 和 Parquet 等列式存储格式，通过在列级别进行数据分析，实现了对结构化数据的高效去重。

\section{模型压缩技术}
随着深度学习模型规模的急剧扩张，模型压缩技术已成为提升部署效率、降低存储与计算成本的关键。主流技术可分为两大类：

\textbf{网络剪枝}（Pruning）旨在通过移除神经网络中的冗余参数来压缩模型。
\begin{itemize}
    \item \textbf{非结构化剪枝}：移除单个不重要的权重，通常依据权重大小或对损失函数的影响来判断。虽然灵活，但可能导致不规则的稀疏矩阵，不利于硬件加速。
    \item \textbf{结构化剪枝}：在更高粒度上（如神经元、通道或层）进行操作，保持了网络的规整结构，便于在通用硬件上高效推理。
\end{itemize}

\textbf{模型量化}（Quantization）通过降低权重和激活值的数值精度来压缩模型并加速计算。
\begin{itemize}
    \item \textbf{训练后量化}（Post-Training Quantization, PTQ）：无需重新训练，直接对已训练好的 FP32 模型进行量化。实现简单，但可能带来精度损失。
    \item \textbf{量化感知训练}（Quantization-Aware Training, QAT）：在训练过程中模拟量化效应，使模型能够适应低精度表示，通常能取得更好的性能。
\end{itemize}

然而，上述剪枝与量化技术属于\textbf{有损压缩}，且严重依赖模型的训练过程。在云存储场景中，用户上传的通常是预训练好的模型文件，并且期望无损存储，因此无法直接应用这些技术。

通用无损压缩算法（如 Gzip）在处理模型文件时效果不佳，因为它们难以有效处理模型参数中普遍存在的数值冗余。针对这一挑战，研究者提出了多种专用的无损模型压缩方法。例如，ZipNN~\cite{hershcovitch2025zipnn}通过重排浮点数的位平面来提升通用压缩器的效率；ZipLLM~\cite{wang2026zipllm}则利用模型家族内的高度相似性，通过比特位异或操作生成稀疏的差异文件；而 FM-Delta~\cite{ning2024fm}专注于模型更新场景，通过计算版本间的增量差异进行压缩。

这些专用方法的局限性在于其高度的“模型感知”特性，它们深度依赖于 LLM 数据的特定结构（如张量布局、浮点数格式等）。若将它们应用于文档、图片等传统用户数据，不仅无法发挥作用，甚至可能因引入额外开销而导致压缩后体积增大。因此，这些技术无法作为通用算法直接集成到处理混合数据类型的云存储去重工作流中。

\section{本章小结}
本章首先系统回顾了云存储服务的三种主要类型——对象存储、文件存储与块存储，并阐述了其分层架构及成本构成，明确了数据缩减技术在云存储中的核心经济价值。
接着，深入剖析了传统重复数据删除系统的工作原理，详细探讨了数据切块（固定大小与内容定义）、哈希指纹与索引、以及数据压缩等关键技术环节，并指出了其在面对新型数据（如 AI 模型）时暴露出的局限性。
随后，本章介绍了为克服传统去重技术限制而发展的细粒度去重技术，特别是增量压缩的原理、应用与挑战，同时也提及了其他如 Burst 等面向特定场景的细粒度去重方法。
最后，我们概述了模型压缩技术的发展现状，区分了有损（剪枝、量化）与无损压缩方法，并重点分析了 ZipLLM 等专用无损压缩技术在云存储环境中的适用性限制。
总体而言，本章通过对相关技术的全面梳理，为后续章节中提出的、旨在统一处理混合数据类型的细粒度新型数据缩减方法，奠定了坚实的理论基础，并清晰地界定了当前研究的技术空白与需求。