% !TEX root = ../main.tex

\chapter{绪 论}


\section{研究背景与意义}
云计算技术的成熟与普及，已深刻地重塑了现代信息技术产业的格局。作为云计算服务的基石，云存储系统承担着
为全球海量用户提供可靠、可扩展数据存储能力的核心使命。随着数字化转型浪潮席卷各行各业，以及大数据、
人工智能等技术的广泛应用，全球数据总量正经历爆炸式增长。企业级应用、个人存储、物联网设备以及科学
计算等场景，每时每刻都在向云端注入天量数据，使得存储成本成为云服务厂商运营支出中最主要的构成部分之一。

为了有效应对大规模数据带来的成本压力，数据缩减技术——尤其是重复数据删除与压缩——自诞生以来便成为云存储系
统中不可或缺的核心模块。其中，基于固定长度分块的重复数据删除技术因其实现简单、性能稳定而得到最为广泛
的应用。其技术范式可概括为三个核心步骤：首先，将连续的数据流切割成固定大小的数据块（典型大小为4KB至
128KB）；继而，为每个数据块计算一个唯一的密码学哈希值（如SHA-1或SHA-256），作为该数据块的“指纹”；
最后，在系统内维护一个全局的指纹库，通过比对指纹来识别重复数据块：若指纹已存在，则仅存储一个指向
已有数据块的元数据指针；若指纹不存在，则对该新数据块进行压缩后存入物理介质，并将其指纹加入库中。这套
“分块—指纹—比对”的技术体系，在过去以文档、电子邮件、虚拟机镜像等文本类数据为主的时代取得了辉煌的成
功，能够在这些场景下实现30\%至50\%乃至更高的存储空间节省，为云存储的规模化与商业化奠定了坚实的技术基础。

然而，近年来，随着人工智能，特别是深度学习技术的突破性进展，云存储所承载的工
作负载正在发生一场静默却深刻的革命。一个日益凸显的趋势是：存储池中的数据构成正从相对单一的文本类型
，转变为传统数据与AI模型数据共存的混合体。以.pt, .safetensors等格式保存的模型参数文件，其体积动
辄达到数百GB甚至TB级别，在总存储容量中的占比急剧攀升，已成为不可忽视的“新常态”数据。与此同时，为了
追求极高的I/O吞吐性能以满足高性能计算和大数据分析的需求，现代存储系统倾向于采用更大的数据块尺寸，
MB级别的数据块处理已在生产环境中屡见不鲜。

正是在这种新的数据特征下，沿用多年的传统数据缩减技术体系开始显现疲态，其效能出现断崖式下滑。在
不少混合负载场景中，整体的存储节省率已从过去的50\%左右暴跌至10\%以下。

首先，对于海量的AI模型数据，传统去重与压缩技术双双“失灵”。

去重失灵：模型文件由亿万级相互独立的浮点数张量构成，其数据内容具有高度的随机性和唯一性。
当系统将其切分为固定大小的块时，几乎每一个数据块都会拥有一个独一无二的指纹，这使得基于全局指
纹匹配的重复数据删除机制彻底失效，去重率趋近于零。

压缩失灵：诸如LZ4、Zlib、Zstandard等通用压缩算法，其核心原理是寻找并消除字节序列层面的
重复模式。然而，模型数据的冗余并不体现在连续的字节流中。其真正的冗余源于浮点数的数值分布特征与内
部结构。具体而言，在32位单精度浮点数中，其二进制表示由符号位、指数位和尾数位构成。在训练好
的模型中，绝大部分权重参数的数值都集中在某个较小的区间内（例如[-2, 2]），这导致其指数位的前几位
高度相似甚至完全相同。此外，由于量化或模型结构带来的规律性，在字节层面上可能呈现出以2字节或
4字节为周期的“循环相似”模式。(bit熵值)这种高位值重复、数值分布集中的“数值冗余”，是传统字节级压缩算法
无法理解和利用的。

其次，对于尺寸日益增大的传统数据块，传统去重技术的效果也大打折扣。

将数据块从KB级提升到MB级，虽然提升了I/O吞吐的连续性，但也极大地降低了找到两个完全一致的大数据
块的概率。然而，这并不意味着大块数据内部没有冗余。恰恰相反，在日志文件、备份集、版本库等场景中，
大块数据内部往往存在着大量局部性、连续性的重复片段（例如，同一时段产生的相同错误日志、文档中重复
出现的模板段落、代码库中相同的函数实现等）。传统以整个块为单位的“粗粒度”去重，造成了
冗余识别能力的巨大浪费。


综上所述，当前云存储数据缩减技术所面临的困境，其症结并非在于数据本身没有压缩空间，而是在
于传统技术冗余识别的“粒度”不足。无论是模型数据中隐含的“数值冗余”，还是大块传统数据中存在的
“局部连续冗余”，都超越了传统块级指纹比对技术的感知范围。因此，突破这一瓶颈的关键路径在于，
从“块级”的粗放式处理，迈向 “细粒度冗余识别” 的新范式。

然而，一个随之而来的严峻挑战是：模型数据与非模型大块数据所蕴含的细粒度冗余，其类型、结构与产
生机理截然不同。针对一种冗余类型的优化算法，很可能对另一种类型的数据产生“负优化”效应（负优化图）。例如，
专门为提取浮点数指数位冗余而设计的编码算法，如果被错误地应用于文本数据，由于其破坏了文本中原有的
字节序列重复模式，反而可能导致压缩率低于不使用任何优化；反之，专为发现文本数据块内部局部重复而设计的算
法，对模型数据则完全无效，反而增加了元数据存储和计算开销。这种在混合数据流中优化策略相互干扰，使得简单地堆砌多种
优化算法变得不可行。

至此，问题的全貌已然清晰：我们既需要更细粒度的冗余识别技术来挖掘新型负载中的压缩潜力，又必
须避免不同优化技术在混合场景下的相互干扰。面对这一两难问题，本研究提出了一条核心解决思路：“
鉴别数据，分而治之”。

具体而言，我们设想在传统的数据处理流水线中，增加一个智能的、轻量级的数据鉴别层。该层的任务是
实时地、精准地对每一个数据块进行分析，判断其更接近于“模型数据”还是“非模型传统数据”。在此基础上
，系统将实施 “分治”策略 ：对鉴别出的模型数据块，采用专门为之设计的浮点数冗余提取与编码技术，
消除其数值冗余；对鉴别出的传统数据块，则采用增强的、面向块内局部连续重复的检测与消除技术。通
过这种“先鉴别，后处理”的协同架构，我们有望在不过分影响系统吞吐性能的前提下，精准地对不同类型的
数据施加最有效的优化手段，从而系统性地提升混合负载下的整体去重率，攻克传统技术在AI时代面临的技术瓶颈。

\section{国内外研究现状}

\section{本文研究工作与主要贡献}

\section{论文结构}
% \subsection{三级标题}

% \subsubsection{四级标题}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
% incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
% nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
% Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu
% fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in
% culpa qui officia deserunt mollit anim id est laborum.

% \section{脚注}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
% incididunt ut labore et dolore magna aliqua. \footnote{Ut enim ad minim veniam,
% quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo
% consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum
% dolore eu fugiat nulla pariatur.}

% \section{字体}

% 上海交通大学是我国历史最悠久的高等学府之一，是教育部直属、教育部与上海市共建的全
% 国重点大学，是国家“七五”、“八五”重点建设和“211 工程”、“985 工程”的首批建
% 设高校。经过 115 年的不懈努力，上海交通大学已经成为一所“综合性、研究型、国际化”
% 的国内一流、国际知名大学，并正在向世界一流大学稳步迈进。 

